{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dc49445-c853-4231-87a7-6323753f9696",
   "metadata": {},
   "outputs": [],
   "source": [
    "from Scripts.essentials import *\n",
    "from Scripts.vars import *\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import roc_curve, auc, balanced_accuracy_score\n",
    "from sklearn.metrics import precision_recall_curve, average_precision_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d73afc4f-d6f2-495d-ae32-5579ad9427ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_axis = pd.read_excel(\"Data/RamanFreqTable.xlsx\")[\"Wave\"].values\n",
    "\n",
    "print(\"Using:\\n\\t n_folds = \", n_folds, \"\\n\\t learning rate= \", lr, \"\\n\\t batch size = \", batch_size, \"\\n\\t epochs = \", epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66d2718d-de81-4ef3-9ca7-fa0731ac0011",
   "metadata": {},
   "outputs": [],
   "source": [
    "m = make_model()\n",
    "m.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3f1dc47-e44b-4137-b3fe-e4a7f5a7cabb",
   "metadata": {},
   "outputs": [],
   "source": [
    "m = make_split_model()\n",
    "m.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a8119f7-2997-44ca-aa22-c2cf19bb8ad6",
   "metadata": {},
   "outputs": [],
   "source": [
    "m = make_encoder()\n",
    "m.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01413051-1618-4ab4-a299-c82dc0c29638",
   "metadata": {},
   "source": [
    "# Simple example of how applying preprocessing to data can help with the spectrum effect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9666b070-8219-47ff-9194-0572b4d6dc2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "# Axes for plotting comparisons\n",
    "\n",
    "\n",
    "\n",
    "########################\n",
    "# Train on raw data\n",
    "########################\n",
    "\n",
    "\n",
    "p = \"Data/\"\n",
    "X = np.concatenate([np.load(p + \"train_x.npy\"), np.load(p + \"val_x.npy\"), np.load(p + \"test_x.npy\")])\n",
    "y = np.concatenate([np.load(p + \"train_y.npy\"), np.load(p + \"val_y.npy\"), np.load(p + \"test_y.npy\")])\n",
    "\n",
    "HF_1887 = X[np.argmax(y, axis = 1) == 10] # Get sample spectra\n",
    "d = HF_1887\n",
    "min_ = np.min(d, axis = 0)\n",
    "max_ = np.max(d, axis = 0)\n",
    "sd = np.std(d, axis = 0)\n",
    "mean = np.mean(d, axis = 0)\n",
    "\n",
    "\n",
    "plt.rcParams.update({'font.size': 40})\n",
    "plt.rcParams[\"font.family\"] = \"Times New Roman\"\n",
    "plt.fill_between(x_axis, mean - sd, mean + sd, alpha = 1, color = \"Red\")\n",
    "plt.plot(x_axis, mean, linestyle = \"--\", color = \"Black\")\n",
    "plt.xticks(x_axis[[0, int(1737/2), 1737]])\n",
    "plt.rcParams.update({'font.size': 40})\n",
    "plt.xlabel(r\"Wavenumber (cm$^{-1}$)\")\n",
    "plt.savefig(\"Images/(RAW)SolveSpectrumEffect_1887.png\", format=\"png\", transparent = True,\n",
    "                    dpi = 300,\n",
    "                    bbox_inches='tight',\n",
    "                    pad_inches=0.5)\n",
    "plt.show()\n",
    "\n",
    "# Get the other sample\n",
    "\n",
    "HF_2070 = X[np.argmax(y, axis = 1) == 17]\n",
    "d = HF_2070\n",
    "min_ = np.min(d, axis = 0)\n",
    "max_ = np.max(d, axis = 0)\n",
    "sd = np.std(d, axis = 0)\n",
    "mean = np.mean(d, axis = 0)\n",
    "\n",
    "plt.rcParams.update({'font.size': 40})\n",
    "plt.rcParams[\"font.family\"] = \"Times New Roman\"\n",
    "plt.fill_between(x_axis, mean - sd, mean + sd, alpha = 1, color = \"Red\")\n",
    "plt.plot(x_axis, mean, linestyle = \"--\", color = \"Black\")\n",
    "plt.xticks(x_axis[[0, int(1737/2), 1737]])\n",
    "plt.xlabel(r\"Wavenumber (cm$^{-1}$)\")\n",
    "plt.savefig(\"Images/(RAW)SolveSpectrumEffect_2070.png\", format=\"png\", transparent = True,\n",
    "                    dpi = 300,\n",
    "                    bbox_inches='tight',\n",
    "                    pad_inches=0.5)\n",
    "plt.show()\n",
    "\n",
    "# Plot comparison between populations\n",
    "d = HF_1887\n",
    "min_1 = np.min(d, axis = 0)\n",
    "max_1 = np.max(d, axis = 0)\n",
    "sd1 = np.std(d, axis = 0)\n",
    "mean1 = np.mean(d, axis = 0)\n",
    "\n",
    "d = HF_2070\n",
    "min_2 = np.min(d, axis = 0)\n",
    "max_2 = np.max(d, axis = 0)\n",
    "sd2 = np.std(d, axis = 0)\n",
    "mean2 = np.mean(d, axis = 0)\n",
    "\n",
    "plt.rcParams.update({'font.size': 40})\n",
    "plt.rcParams[\"font.family\"] = \"Times New Roman\"\n",
    "plt.fill_between(x_axis, mean1 - sd1, mean1 + sd1, alpha = 0.5, color = \"Cyan\", label = \"HF-1887\")\n",
    "plt.plot(x_axis, mean1, linestyle = \"--\", color = \"blue\", label = \"Mean(HF-1887)\")\n",
    "plt.fill_between(x_axis, mean2 - sd2, mean2 + sd2, alpha = 0.5, color = \"Magenta\", label = \"HF-2070\")\n",
    "plt.plot(x_axis, mean2, linestyle = \"--\", color = \"Red\", label = \"Mean(HF-2070)\")\n",
    "plt.xticks(x_axis[[0, int(1737/2), 1737]])\n",
    "plt.xlabel(r\"Wavenumber (cm$^{-1}$)\")\n",
    "plt.legend(fontsize=20)\n",
    "plt.savefig(\"Images/(RAW)SolveSpectrumEffect_Comparison.png\", format=\"png\", transparent = True,\n",
    "                    dpi = 300,\n",
    "                    bbox_inches='tight',\n",
    "                    pad_inches=0.5)\n",
    "plt.show()\n",
    "\n",
    "d = np.concatenate([HF_1887, HF_2070])\n",
    "l = np.concatenate([np.zeros(len(HF_1887)), np.ones(len(HF_2070))])\n",
    "\n",
    "print(\"Sample ratio:\", str(len(HF_1887)/(len(HF_2070))))\n",
    "\n",
    "# Get the folds\n",
    "organizer = StratifiedKFold(n_splits=n_folds, shuffle = True, random_state = 42)\n",
    "folds = organizer.split(d, l)\n",
    "\n",
    "\n",
    "mean_fpr = np.linspace(0, 1, 200)\n",
    "tprs = []\n",
    "aucs = []\n",
    "\n",
    "model = make_model(lr = lr, out_dim = 2)\n",
    "model.summary()\n",
    "plt.figure(figsize = (10, 10))\n",
    "for en, fold in enumerate(folds):\n",
    "    train_ix = fold[0]\n",
    "    val_ix = fold[1]\n",
    "    \n",
    "    d_train = d[train_ix]\n",
    "    l_train = l[train_ix]\n",
    "    d_val = d[val_ix]\n",
    "    l_val = l[val_ix]\n",
    "    \n",
    "    ix = np.arange(len(d_train))\n",
    "    np.random.shuffle(ix)\n",
    "    d_train = d_train[ix]\n",
    "    l_train = l_train[ix]\n",
    "    \n",
    "    \n",
    "    # Create and train the model\n",
    "    reset_seed()\n",
    "    model = make_model(lr = lr, out_dim = 2)\n",
    "    hist1 = model.fit(d_train, l_train, epochs = epochs, batch_size = batch_size, validation_data = [d_val, l_val])\n",
    "    \n",
    "    np.save(\"Results/InitialTest_Raw_hist_\"+str(en)+\".npy\", hist1.history)\n",
    "    model.save_weights(\"Models/InitialTest_Raw_Model_\"+str(en)+\".npy\")\n",
    "\n",
    "    pred = model.predict(d_val)\n",
    "    print(pred.shape)\n",
    "    prob = pred[:, 1]\n",
    "    fpr, tpr, _ = roc_curve(l_val, prob)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    aucs.append(roc_auc)\n",
    "\n",
    "    interp_tpr = np.interp(mean_fpr, fpr, tpr)\n",
    "    interp_tpr[0] = 0.0\n",
    "    tprs.append(interp_tpr)\n",
    "\n",
    "    plt.plot(fpr, tpr, lw=1, alpha=0.3)#, label=f\"Fold {en} AUC={roc_auc:.2f}\")\n",
    "\n",
    "# ---- Mean ROC ----\n",
    "mean_tpr = np.mean(tprs, axis=0)\n",
    "mean_tpr[-1] = 1.0\n",
    "mean_auc = auc(mean_fpr, mean_tpr)\n",
    "\n",
    "# ---- 95% CI band ----\n",
    "std_tpr = np.std(tprs, axis=0)\n",
    "tpr_upper = np.minimum(mean_tpr + 1.96 * std_tpr, 1)\n",
    "tpr_lower = np.maximum(mean_tpr - 1.96 * std_tpr, 0)\n",
    "\n",
    "plt.plot(mean_fpr, mean_tpr, color=\"black\", lw=2,\n",
    "         label=f\"Mean ROC (AUC={mean_auc:.3f})\")\n",
    "\n",
    "plt.fill_between(mean_fpr, tpr_lower, tpr_upper,\n",
    "                 color=\"gray\", alpha=0.25,\n",
    "                 label=\"95% CI\")\n",
    "\n",
    "# ---- Decorations ----\n",
    "plt.plot([0, 1], [0, 1], \"--\", color=\"gray\")\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "plt.legend(fontsize = 20)\n",
    "plt.savefig(\"Images/(Raw)FinalROC.png\", format=\"png\", transparent = True,\n",
    "                        dpi = 300,\n",
    "                        bbox_inches='tight',\n",
    "                        pad_inches=0.5)\n",
    "plt.show()\n",
    "########################\n",
    "# Train on manual data\n",
    "########################\n",
    "\n",
    "\n",
    "fig, ax_spectra = plt.subplots(1)\n",
    "\n",
    "X = np.concatenate([np.load(p + \"train_x_MANUAL.npy\"), np.load(p + \"val_x_MANUAL.npy\"), np.load(p + \"test_x_MANUAL.npy\")])\n",
    "y = np.concatenate([np.load(p + \"train_y.npy\"), np.load(p + \"val_y.npy\"), np.load(p + \"test_y.npy\")])\n",
    "HF_1887 = X[np.argmax(y, axis = 1) == 10]\n",
    "d = HF_1887\n",
    "min_ = np.min(d, axis = 0)\n",
    "max_ = np.max(d, axis = 0)\n",
    "sd = np.std(d, axis = 0)\n",
    "mean = np.mean(d, axis = 0)\n",
    "\n",
    "plt.rcParams.update({'font.size': 40})\n",
    "plt.rcParams[\"font.family\"] = \"Times New Roman\"\n",
    "plt.fill_between(x_axis, mean - sd, mean + sd, alpha = 1, color = \"Red\")\n",
    "plt.plot(x_axis, mean, linestyle = \"--\", color = \"black\")\n",
    "plt.plot(x_axis, mean, linestyle = \"--\", color = \"black\")\n",
    "plt.xticks(x_axis[[0, int(1737/2), 1737]])\n",
    "plt.xlabel(r\"Wavenumber (cm$^{-1}$)\")\n",
    "plt.savefig(\"Images/(MANUAL)SolveSpectrumEffect_1887(prep).png\", format=\"png\", transparent = True,\n",
    "                    dpi = 300,\n",
    "                    bbox_inches='tight',\n",
    "                    pad_inches=0.5)\n",
    "plt.show()\n",
    "\n",
    "HF_2070 = X[np.argmax(y, axis = 1) == 17]\n",
    "d = HF_2070\n",
    "min_ = np.min(d, axis = 0)\n",
    "max_ = np.max(d, axis = 0)\n",
    "sd = np.std(d, axis = 0)\n",
    "mean = np.mean(d, axis = 0)\n",
    "\n",
    "plt.rcParams.update({'font.size': 40})\n",
    "plt.rcParams[\"font.family\"] = \"Times New Roman\"\n",
    "plt.fill_between(x_axis, mean - sd, mean + sd, alpha = 1, color = \"Red\")\n",
    "plt.fill_between(x_axis, mean - sd, mean + sd, alpha = 1, color = \"Red\")\n",
    "plt.plot(x_axis, mean, linestyle = \"--\", color = \"black\")\n",
    "plt.xticks(x_axis[[0, int(1737/2), 1737]])\n",
    "plt.xlabel(r\"Wavenumber (cm$^{-1}$)\")\n",
    "plt.savefig(\"Images/(MANUAL)SolveSpectrumEffect_2070(prep).png\", format=\"png\", transparent = True,\n",
    "                    dpi = 300,\n",
    "                    bbox_inches='tight',\n",
    "                    pad_inches=0.5)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# Plot comparison between populations\n",
    "d = HF_1887\n",
    "min_1 = np.min(d, axis = 0)\n",
    "max_1 = np.max(d, axis = 0)\n",
    "sd1 = np.std(d, axis = 0)\n",
    "mean1 = np.mean(d, axis = 0)\n",
    "\n",
    "d = HF_2070\n",
    "min_2 = np.min(d, axis = 0)\n",
    "max_2 = np.max(d, axis = 0)\n",
    "sd2 = np.std(d, axis = 0)\n",
    "mean2 = np.mean(d, axis = 0)\n",
    "\n",
    "plt.rcParams.update({'font.size': 40})\n",
    "plt.rcParams[\"font.family\"] = \"Times New Roman\"\n",
    "plt.fill_between(x_axis, mean1 - sd1, mean1 + sd1, alpha = 0.5, color = \"Cyan\", label = \"HF-1887\")\n",
    "plt.plot(x_axis, mean1, linestyle = \"--\", color = \"blue\", label = \"Mean(HF-1887)\")\n",
    "plt.fill_between(x_axis, mean2 - sd2, mean2 + sd2, alpha = 0.5, color = \"Magenta\", label = \"HF-2070\")\n",
    "plt.plot(x_axis, mean2, linestyle = \"--\", color = \"Red\", label = \"Mean(HF-2070)\")\n",
    "plt.xticks(x_axis[[0, int(1737/2), 1737]])\n",
    "plt.xlabel(r\"Wavenumber (cm$^{-1}$)\")\n",
    "plt.legend(fontsize=20)\n",
    "plt.savefig(\"Images/(MANUAL)SolveSpectrumEffect_Comparison.png\", format=\"png\", transparent = True,\n",
    "                    dpi = 300,\n",
    "                    bbox_inches='tight',\n",
    "                    pad_inches=0.5)\n",
    "plt.show()\n",
    "\n",
    "d = np.concatenate([HF_1887, HF_2070])\n",
    "l = np.concatenate([np.zeros(len(HF_1887)), np.ones(len(HF_2070))])\n",
    "\n",
    "print(\"Sample ratio:\", str(len(HF_1887)/(len(HF_2070))))\n",
    "\n",
    "# Get the folds\n",
    "organizer = StratifiedKFold(n_splits=n_folds)\n",
    "folds = organizer.split(d, l)\n",
    "\n",
    "\n",
    "mean_fpr = np.linspace(0, 1, 200)\n",
    "tprs = []\n",
    "aucs = []\n",
    "\n",
    "plt.figure(figsize = (10, 10))\n",
    "for en, fold in enumerate(folds):\n",
    "    train_ix = fold[0]\n",
    "    val_ix = fold[1]\n",
    "    \n",
    "    d_train = d[train_ix]\n",
    "    l_train = l[train_ix]\n",
    "    d_val = d[val_ix]\n",
    "    l_val = l[val_ix]\n",
    "    \n",
    "    ix = np.arange(len(d_train))\n",
    "    np.random.shuffle(ix)\n",
    "    d_train = d_train[ix]\n",
    "    l_train = l_train[ix]\n",
    "    \n",
    "    \n",
    "    # Create and train the model\n",
    "    reset_seed()\n",
    "    model = make_model(lr = lr, out_dim = 2)\n",
    "    hist1 = model.fit(d_train, l_train, epochs = epochs, batch_size = batch_size, validation_data = [d_val, l_val])\n",
    "    \n",
    "    np.save(\"Results/InitialTest_MANUAL_hist_\"+str(en)+\".npy\", hist1.history)\n",
    "    model.save_weights(\"Models/InitialTest_MANUAL_Model_\"+str(en)+\".npy\")\n",
    "\n",
    "    pred = model.predict(d_val)\n",
    "    prob = pred[:, 1]\n",
    "    fpr, tpr, _ = roc_curve(l_val, prob)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    aucs.append(roc_auc)\n",
    "\n",
    "    interp_tpr = np.interp(mean_fpr, fpr, tpr)\n",
    "    interp_tpr[0] = 0.0\n",
    "    tprs.append(interp_tpr)\n",
    "\n",
    "    plt.plot(fpr, tpr, lw=1, alpha=0.3)#, label=f\"Fold {en} AUC={roc_auc:.2f}\")\n",
    "\n",
    "# ---- Mean ROC ----\n",
    "mean_tpr = np.mean(tprs, axis=0)\n",
    "mean_tpr[-1] = 1.0\n",
    "mean_auc = auc(mean_fpr, mean_tpr)\n",
    "\n",
    "# ---- 95% CI band ----\n",
    "std_tpr = np.std(tprs, axis=0)\n",
    "tpr_upper = np.minimum(mean_tpr + 1.96 * std_tpr, 1)\n",
    "tpr_lower = np.maximum(mean_tpr - 1.96 * std_tpr, 0)\n",
    "\n",
    "plt.plot(mean_fpr, mean_tpr, color=\"black\", lw=2,\n",
    "         label=f\"Mean ROC (AUC={mean_auc:.2f})\")\n",
    "\n",
    "plt.fill_between(mean_fpr, tpr_lower, tpr_upper,\n",
    "                 color=\"gray\", alpha=0.25,\n",
    "                 label=\"95% CI\")\n",
    "\n",
    "# ---- Decorations ----\n",
    "plt.plot([0, 1], [0, 1], \"--\", color=\"gray\")\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "plt.legend(fontsize = 20)\n",
    "plt.savefig(\"Images/(MANUAL)FinalROC.png\", format=\"png\", transparent = True,\n",
    "                        dpi = 300,\n",
    "                        bbox_inches='tight',\n",
    "                        pad_inches=0.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0f06e56-6a18-415d-af60-d0db40ff6dea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_training_comparison(key, title = \"Raw\", ylims = [-0.05, 4.05]):\n",
    "\n",
    "    plt.rcParams.update({'font.size': 40})\n",
    "    plt.rcParams[\"font.family\"] = \"Times New Roman\"\n",
    "    metrics = []\n",
    "    for en in range(n_folds):\n",
    "        \n",
    "        hist = np.load(\"Results/InitialTest_\"+title+\"_hist_\"+str(en)+\".npy\", allow_pickle = True).item()\n",
    "        metrics.append(hist[key])\n",
    "    \n",
    "    metrics = np.array(metrics)\n",
    "    \n",
    "\n",
    "    plt.plot(metrics.T, alpha = 0.1, color = \"blue\")\n",
    "    #plt.fill_between(np.arange(epochs), np.min(metrics, axis = 0), np.max(metrics, axis = 0), color = \"blue\", alpha = 0.2)\n",
    "    mean = np.mean(metrics, axis = 0)\n",
    "    std = np.std(metrics, axis = 0)\n",
    "    plt.fill_between(np.arange(epochs), mean - std, mean + std, color = \"blue\", alpha = 0.2, title = \"Train Mean ± Std\")\n",
    "    plt.plot(mean, color = \"black\", label = \"Train, Final Mean:\" + str(np.round(mean[-1], 2)))\n",
    "    \n",
    "    \n",
    "    metrics = []\n",
    "    for en in range(n_folds):\n",
    "     \n",
    "        hist = np.load(\"Results/InitialTest_\"+title+\"_hist_\"+str(en)+\".npy\", allow_pickle = True).item()\n",
    "        metrics.append(hist[\"val_\"+key])\n",
    "    \n",
    "    metrics = np.array(metrics)\n",
    "    \n",
    "    plt.plot(metrics.T, alpha = 0.1, color = \"red\")\n",
    "    \n",
    "    mean = np.mean(metrics, axis = 0)\n",
    "    std = np.std(metrics, axis = 0)\n",
    "    plt.fill_between(np.arange(epochs), mean - std, mean + std, color = \"red\", alpha = 0.2, title = \"Validation Mean ± Std\")\n",
    "    plt.plot(mean, color = \"black\", ls = \"--\", label = \"Val., , Final Mean:\" + str(np.round(mean[-1], 2)))\n",
    "    plt.legend(fontsize = 20)\n",
    "\n",
    "    plt.ylim(ylims)\n",
    "\n",
    "    plt.savefig(\"Images/\"+title+\"_InitialLearning_\"+key+\".png\", format=\"png\", transparent = True,\n",
    "                    dpi = 1000,\n",
    "                    bbox_inches='tight',\n",
    "                    pad_inches=0.5)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6553fd66-d520-47cb-9931-48c0f1fbcc35",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.stats as stats\n",
    "conf_level = 0.95\n",
    "\n",
    "\n",
    "key = \"loss\"\n",
    "\n",
    "plot_training_comparison(key, title = \"Raw\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7889ed38-eb5b-4360-9c49-411d777706f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "key = \"loss\"\n",
    "\n",
    "plot_training_comparison(key, title = \"MANUAL\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a550d932-d2fc-45d4-a16b-51beb6e69d47",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "key = \"accuracy\"\n",
    "\n",
    "plot_training_comparison(key, title = \"Raw\", ylims = [-0.05, 1.05])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35d5b069-9c43-4e9a-bf3c-4d25546f49c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "key = \"accuracy\"\n",
    "\n",
    "plot_training_comparison(key, title = \"MANUAL\", ylims = [-0.05, 1.05])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0674308-ca20-4009-bd88-93a8c52f1ce3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import label_binarize\n",
    "\n",
    "def TrainModel(train_x, train_y, train_lgm, val_x, val_y, val_lgm, en, title):\n",
    "    \n",
    "    # Compute the weights for each category type\n",
    "    counts = np.bincount(train_y)\n",
    "    class_weights = np.sqrt((1/(counts/np.max(counts))))\n",
    "    \n",
    "    cw_id = {}\n",
    "    \n",
    "    for i in range(len(class_weights)):\n",
    "        cw_id[i] = class_weights[i]\n",
    "        \n",
    "    counts = np.bincount(train_lgm)\n",
    "    class_weights = np.sqrt((1/(counts/np.max(counts))))\n",
    "    \n",
    "    cw_lgm = {}\n",
    "    \n",
    "    for i in range(len(class_weights)):\n",
    "        cw_lgm[i] = class_weights[i]\n",
    "    \n",
    "    # Get sample wise weights to enable multi-output-balancing\n",
    "    sw_id = np.array([cw_id[c] for c in train_y])\n",
    "    sw_lgm = np.array([cw_lgm[c] for c in train_lgm])\n",
    "\n",
    "\n",
    "    # Create and train the model\n",
    "    reset_seed()\n",
    "    sample_model = make_split_model(lr, out_dims = [len(np.unique(train_y)), len(np.unique(train_lgm))],\n",
    "                                losses = [\"sparse_categorical_crossentropy\", \"sparse_categorical_crossentropy\"])\n",
    "    sample_model.summary()\n",
    "\n",
    "    hist = sample_model.fit(train_x, {'ID': train_y, 'LGm': train_lgm}, sample_weight={'ID': sw_id, 'LGm': sw_lgm},\n",
    "                            epochs = epochs, batch_size = batch_size, validation_data = (val_x, [val_y, val_lgm]))\n",
    "\n",
    "    # Save history and model parameters\n",
    "    np.save(\"Results/FullTest_\"+title+\"_hist_\"+str(en)+\".npy\", hist.history)\n",
    "    sample_model.save_weights(\"Models/FullTest_\"+title+\"_Model_\"+str(en)+\".h5\")\n",
    "\n",
    "    return sample_model\n",
    "\n",
    "def sample_evaluation(sample_model, train_x, train_y, train_lgm, val_x, val_y, val_lgm, en, title, verbose = False):\n",
    "\n",
    "    # get performance on each sample\n",
    "    sample_dict = {}\n",
    "    \n",
    "    num_patients = len(np.unique(train_y))\n",
    "    for i in range(num_patients):\n",
    "    \n",
    "        # Get data from class i of the training set\n",
    "        d = np.squeeze(train_x[train_y == i])\n",
    "        y = np.squeeze(train_y[train_y == i])\n",
    "        lgm = np.squeeze(train_lgm[train_y == i])\n",
    "        _, train_loss, _, train_acc, train_acc_lgm  = sample_model.evaluate(d, [y, lgm], batch_size = 256)\n",
    "        train_size = int(len(d))\n",
    "    \n",
    "        # Get data from class i of the validation set, these spectra have not been seen before\n",
    "        d = np.squeeze(val_x[val_y == i])\n",
    "        y = np.squeeze(val_y[val_y == i])\n",
    "        lgm = np.squeeze(val_lgm[val_y == i])\n",
    "        _, val_loss, _, val_acc, val_acc_lgm  = sample_model.evaluate(d, [y, lgm], batch_size = 256)\n",
    "        val_size = int(len(d))\n",
    "\n",
    "    \n",
    "        sample_dict[i] = np.array([train_acc, train_acc_lgm, train_size, val_acc, val_acc_lgm, val_size])\n",
    "    \n",
    "    # Save the metrics in a dataframe\n",
    "    header = [\"Sample ID\", \"Train Accuracy\", \"Train LGm Accuracy\", \"Train Size\",\n",
    "              \"Validation Accuracy\", \"Validation LGm Accuracy\", \"Validation Size\"]\n",
    "    \n",
    "    df = pd.DataFrame.from_dict(sample_dict, columns = header[1:],\n",
    "                                            orient = \"index\")\n",
    "    # Round them to two decimals\n",
    "    df = df.round(decimals = 2)\n",
    "    \n",
    "    # Cast the number of spectra to integer, looks nicer than the decimal form with .0 after each number\n",
    "    df[\"Train Size\"] = df[\"Train Size\"].apply(lambda x: int(x))\n",
    "    df[\"Validation Size\"] = df[\"Validation Size\"].apply(lambda x: int(x))\n",
    "    \n",
    "    # Styling for convertion into latex format\n",
    "    df.style.set_table_styles([\n",
    "        {'selector': 'toprule', 'props': ':hline;'},\n",
    "        {'selector': 'midrule', 'props': ':hline;'},\n",
    "        {'selector': 'bottomrule', 'props': ':hline;'},\n",
    "    ], overwrite=False)\n",
    "\n",
    "    if verbose:\n",
    "        # Print the latex table, can be copied into the editor\n",
    "        latex = df.style.format(decimal=',', thousands='.', precision=2).to_latex(clines=\"all;data\",  column_format=\"|l|l|\")\n",
    "        print(latex)\n",
    "    \n",
    "    # Save metrics\n",
    "    np.save(\"Results/\"+title+\"val_accuracy_\"+str(en)+\".npy\", df[\"Validation Accuracy\"].values)\n",
    "    np.save(\"Results/\"+title+\"all_accuracies_\"+str(en)+\".npy\", df.values)\n",
    "    \n",
    "\n",
    "def compute_multiclass_roc(true_labels, pred_probs):\n",
    "    \"\"\"\n",
    "    Handles:\n",
    "    - binary (N,), (N,1)\n",
    "    - multiclass (N,C)\n",
    "    \"\"\"\n",
    "    norm_preds = []\n",
    "    for p in pred_probs:\n",
    "        p = np.asarray(p)\n",
    "        if p.ndim == 1:\n",
    "            p = p.reshape(-1,1)\n",
    "        norm_preds.append(p)\n",
    "\n",
    "    true_labels = [np.asarray(y) for y in true_labels]\n",
    "\n",
    "    # Detect number of classes from TRUE labels\n",
    "    classes = np.unique(true_labels[0])\n",
    "    n_classes = len(classes)\n",
    "\n",
    "    # Binary case\n",
    "    if n_classes == 2:\n",
    "        fprs = []\n",
    "        tprs = []\n",
    "        aucs = []\n",
    "        for y_fold, p_fold in zip(true_labels, norm_preds):\n",
    "            # Ensure p_fold is (N,2)\n",
    "            if p_fold.shape[1] == 1:\n",
    "                # convert sigmoid output to 2-class softmax\n",
    "                p_fold = np.hstack([1-p_fold, p_fold])\n",
    "            fpr, tpr, _ = roc_curve(y_fold, p_fold[:,1])\n",
    "            aucs.append(auc(fpr, tpr))\n",
    "            fprs.append(fpr)\n",
    "            tprs.append(tpr)\n",
    "        return [fprs], [tprs], np.array(aucs), classes\n",
    "\n",
    "    # Multiclass case\n",
    "    fpr_list = [[] for _ in range(n_classes)]\n",
    "    tpr_list = [[] for _ in range(n_classes)]\n",
    "    auc_scores = []\n",
    "\n",
    "    for y_fold, p_fold in zip(true_labels, norm_preds):\n",
    "        # ensure p_fold has enough columns\n",
    "        if p_fold.shape[1] != n_classes:\n",
    "            raise ValueError(f\"Prediction columns {p_fold.shape[1]} != number of classes {n_classes}\")\n",
    "        Y = label_binarize(y_fold, classes=classes)\n",
    "        auc_fold = {}\n",
    "        for k in range(n_classes):\n",
    "            fpr, tpr, _ = roc_curve(Y[:,k], p_fold[:,k])\n",
    "            fpr_list[k].append(fpr)\n",
    "            tpr_list[k].append(tpr)\n",
    "            auc_fold[k] = auc(fpr, tpr)\n",
    "        auc_scores.append(np.mean(list(auc_fold.values())))\n",
    "    return fpr_list, tpr_list, np.array(auc_scores), classes\n",
    "\n",
    "\n",
    "# ========= Plotting function =========\n",
    "def plot_roc_with_folds_and_mean(true_labels, pred_probs, title, color='Black'):\n",
    "    \"\"\"\n",
    "    Plot ROC curves for individual folds (one per fold), \n",
    "    mean curve with 95% CI, and mean AUC ± CI in legend.\n",
    "    Works for binary and multiclass (One-vs-Rest averaged).\n",
    "    \"\"\"\n",
    "    norm_preds = []\n",
    "    for p in pred_probs:\n",
    "        p = np.asarray(p)\n",
    "        if p.ndim == 1:\n",
    "            p = p.reshape(-1,1)\n",
    "        norm_preds.append(p)\n",
    "    \n",
    "    true_labels = [np.asarray(y) for y in true_labels]\n",
    "    classes = np.unique(true_labels[0])\n",
    "    n_classes = len(classes)\n",
    "    \n",
    "    mean_fpr = np.linspace(0,1,500)\n",
    "    all_interp_tprs = []\n",
    "    aucs = []\n",
    "\n",
    "    plt.figure(figsize=(10,10))\n",
    "    \n",
    "    for fold_idx, (y_fold, p_fold) in enumerate(zip(true_labels, norm_preds)):\n",
    "        # Binary\n",
    "        if n_classes == 2:\n",
    "            if p_fold.shape[1] == 1:\n",
    "                p_fold = np.hstack([1-p_fold, p_fold])\n",
    "            fpr, tpr, _ = roc_curve(y_fold, p_fold[:,1])\n",
    "            plt.plot(fpr, tpr, alpha=0.2, lw=1)  # individual fold\n",
    "            interp_tpr = np.interp(mean_fpr, fpr, tpr)\n",
    "            interp_tpr[0] = 0.0\n",
    "            all_interp_tprs.append(interp_tpr)\n",
    "            aucs.append(auc(fpr, tpr))\n",
    "        # Multiclass\n",
    "        else:\n",
    "            if p_fold.shape[1] != n_classes:\n",
    "                raise ValueError(f\"Prediction columns {p_fold.shape[1]} != number of classes {n_classes}\")\n",
    "            Y = label_binarize(y_fold, classes=classes)\n",
    "            # Compute TPR for each class\n",
    "            tprs_per_class = []\n",
    "            aucs_per_class = []\n",
    "            for k in range(n_classes):\n",
    "                fpr, tpr, _ = roc_curve(Y[:,k], p_fold[:,k])\n",
    "                tprs_per_class.append(np.interp(mean_fpr, fpr, tpr))\n",
    "                aucs_per_class.append(auc(fpr, tpr))\n",
    "            # Average across classes → **one curve per fold**\n",
    "            mean_tpr_fold = np.mean(tprs_per_class, axis=0)\n",
    "            plt.plot(mean_fpr, mean_tpr_fold, alpha=0.2, lw=1)  # individual fold\n",
    "            all_interp_tprs.append(mean_tpr_fold)\n",
    "            aucs.append(np.mean(aucs_per_class))\n",
    "\n",
    "    # Compute mean TPR and CI\n",
    "    all_interp_tprs = np.array(all_interp_tprs)\n",
    "    mean_tpr = np.mean(all_interp_tprs, axis=0)\n",
    "    lower_tpr = np.percentile(all_interp_tprs, 2.5, axis=0)\n",
    "    upper_tpr = np.percentile(all_interp_tprs, 97.5, axis=0)\n",
    "    mean_auc = np.mean(aucs)\n",
    "    auc_ci = (np.percentile(aucs, 2.5), np.percentile(aucs, 97.5))\n",
    "    \n",
    "    # Plot mean and CI\n",
    "    plt.plot(mean_fpr, mean_tpr, color=color, lw=2)\n",
    "    plt.fill_between(mean_fpr, lower_tpr, upper_tpr, color=color, alpha=0.2)\n",
    "    plt.plot([0,1],[0,1],'k--', lw=1)\n",
    "    \n",
    "    plt.xlabel(\"False Positive Rate\")\n",
    "    plt.ylabel(\"True Positive Rate\")\n",
    "    delta = max(auc_ci[1]-mean_auc, mean_auc-auc_ci[0])\n",
    "    plt.legend([f\"Mean AUC = {mean_auc:.2f} ± {delta:.2f}\"], loc='lower right', fontsize = 32)\n",
    "\n",
    "    plt.savefig(\"Images/\"+title+\".png\", format=\"png\", transparent = True,\n",
    "                        dpi = 300,\n",
    "                        bbox_inches='tight',\n",
    "                        pad_inches=0.5)\n",
    "    plt.show()\n",
    "    \n",
    "    return mean_fpr, mean_tpr, lower_tpr, upper_tpr, mean_auc, auc_ci\n",
    "\n",
    "def plot_pr_with_folds_and_mean(true_labels, pred_probs, title, color='black'):\n",
    "    \"\"\"\n",
    "    Plot Precision-Recall curves for multiclass (one-vs-rest) with folds:\n",
    "    - individual fold curves (transparent)\n",
    "    - mean curve across folds\n",
    "    - 95% CI\n",
    "    - mean Average Precision (AP) ± CI in legend\n",
    "    \"\"\"\n",
    "    \n",
    "    # Normalize predictions (ensure 2D)\n",
    "    norm_preds = []\n",
    "    for p in pred_probs:\n",
    "        p = np.asarray(p)\n",
    "        if p.ndim == 1:\n",
    "            p = p.reshape(-1,1)\n",
    "        norm_preds.append(p)\n",
    "\n",
    "    true_labels = [np.asarray(y) for y in true_labels]\n",
    "    classes = np.unique(true_labels[0])\n",
    "    n_classes = len(classes)\n",
    "\n",
    "    mean_recall = np.linspace(0, 1, 500)\n",
    "    fold_interp_precisions = []\n",
    "    fold_auc_values = []\n",
    "\n",
    "    plt.figure(figsize=(10, 10))\n",
    "\n",
    "\n",
    "    for y_fold, p_fold in zip(true_labels, norm_preds):\n",
    "\n",
    "        # ---------- BINARY ----------\n",
    "        if n_classes == 2:\n",
    "            if p_fold.shape[1] == 1:\n",
    "                p_fold = np.hstack([1 - p_fold, p_fold])\n",
    "\n",
    "            prec, rec, _ = precision_recall_curve(y_fold, p_fold[:, 1])\n",
    "            pr_auc = auc(rec, prec)\n",
    "\n",
    "            # Plot 1 line per fold\n",
    "            plt.plot(rec, prec, alpha=0.25, lw=1, color=color)\n",
    "\n",
    "            interp_prec = np.interp(mean_recall, rec[::-1], prec[::-1])\n",
    "            fold_interp_precisions.append(interp_prec)\n",
    "            fold_auc_values.append(pr_auc)\n",
    "\n",
    "        # ---------- MULTICLASS ----------\n",
    "        else:\n",
    "            if p_fold.shape[1] != n_classes:\n",
    "                raise ValueError(\"Predict-prob dimensions don't match number of classes!\")\n",
    "\n",
    "            Y = label_binarize(y_fold, classes=classes)\n",
    "\n",
    "            # Compute PR per class, average WITHIN fold\n",
    "            class_interp = []\n",
    "            class_aucs = []\n",
    "\n",
    "            for k in range(n_classes):\n",
    "                prec, rec, _ = precision_recall_curve(Y[:, k], p_fold[:, k])\n",
    "                class_aucs.append(auc(rec, prec))\n",
    "\n",
    "                interp_prec = np.interp(mean_recall, rec[::-1], prec[::-1])\n",
    "                class_interp.append(interp_prec)\n",
    "\n",
    "            # ---- AVERAGE classes → ONE line per fold ----\n",
    "            fold_mean_prec = np.mean(class_interp, axis=0)\n",
    "            fold_mean_auc = np.mean(class_aucs)\n",
    "\n",
    "            # Plot 1 line per fold (not per class)\n",
    "            plt.plot(mean_recall, fold_mean_prec, alpha=0.25, lw=1, color=color)\n",
    "\n",
    "            fold_interp_precisions.append(fold_mean_prec)\n",
    "            fold_auc_values.append(fold_mean_auc)\n",
    "\n",
    "\n",
    "    fold_interp_precisions = np.array(fold_interp_precisions)\n",
    "\n",
    "    mean_prec = fold_interp_precisions.mean(axis=0)\n",
    "    lower_prec = np.percentile(fold_interp_precisions, 2.5, axis=0)\n",
    "    upper_prec = np.percentile(fold_interp_precisions, 97.5, axis=0)\n",
    "\n",
    "    mean_auc = np.mean(fold_auc_values)\n",
    "    ci_low, ci_high = np.percentile(fold_auc_values, [2.5, 97.5])\n",
    "    delta = max(mean_auc - ci_low, ci_high - mean_auc)\n",
    "    \n",
    "\n",
    "\n",
    "    plt.plot(mean_recall, mean_prec, color=color, lw=2)\n",
    "    plt.fill_between(mean_recall, lower_prec, upper_prec, color=color, alpha=0.2)\n",
    "    plt.xlabel(\"Recall\")\n",
    "    plt.ylabel(\"Precision\")\n",
    "    \n",
    "    # Short legend with ± notation\n",
    "\n",
    "    plt.legend([f\"AUC-PR = {mean_auc:.2f} ± {delta:.2f}\"], loc=\"lower left\", fontsize = 32)\n",
    "    plt.savefig(\"Images/\"+title+\".png\", format=\"png\", transparent = True,\n",
    "                        dpi = 300,\n",
    "                        bbox_inches='tight',\n",
    "                        pad_inches=0.5)\n",
    "    plt.show()\n",
    "    \n",
    "    return mean_recall, mean_prec, lower_prec, upper_prec, mean_auc, (ci_low, ci_high)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7c48e3e-1c92-4d67-a026-5e5caa92582a",
   "metadata": {},
   "outputs": [],
   "source": [
    "p = \"Data/\"\n",
    "lgm_all = np.concatenate([np.load(p + \"train_lgm.npy\"), np.load(p + \"val_lgm.npy\"), np.load(p + \"test_lgm.npy\")])\n",
    "y_all = np.concatenate([np.load(p + \"train_y_46.npy\"), np.load(p + \"val_y_46.npy\"), np.load(p + \"test_y_46.npy\")])\n",
    "lgm_all = np.argmax(lgm_all, axis = -1)\n",
    "y_all = np.argmax(y_all, axis = -1)\n",
    "lgm_all = np.where(lgm_all > 2, 0, 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e364c492-62cf-42ab-9a26-499363851802",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.preprocessing import label_binarize\n",
    "\n",
    "gc.collect()\n",
    "\n",
    "# Get and prepare the full dataset (join the static train-val-test splits for the full data)\n",
    "p = \"Data/\"\n",
    "title = \"MANUAL\"\n",
    "X_all = np.concatenate([np.load(p + \"train_x_\"+title+\".npy\"), np.load(p + \"val_x_\"+title+\".npy\"), np.load(p + \"test_x_\"+title+\".npy\")])\n",
    "\n",
    "# Get the folds\n",
    "organizer = StratifiedKFold(n_splits=n_folds, shuffle = True, random_state = 42)\n",
    "folds = organizer.split(X_all, y_all)\n",
    "\n",
    "plt.figure(figsize = (8, 8))\n",
    "for en, fold in enumerate(folds):\n",
    "    train_ix = fold[0]\n",
    "    val_ix = fold[1]\n",
    "\n",
    "    # Training set\n",
    "    train_x = X_all[train_ix]\n",
    "    train_y = y_all[train_ix]\n",
    "    train_lgm = lgm_all[train_ix]\n",
    "\n",
    "    # Validation set\n",
    "    val_x = X_all[val_ix]\n",
    "    val_y = y_all[val_ix]\n",
    "    val_lgm = lgm_all[val_ix]\n",
    "\n",
    "    # Data shuffle\n",
    "    ix = np.arange(len(train_x))\n",
    "    np.random.shuffle(ix)\n",
    "    train_x = train_x[ix]\n",
    "    train_y = train_y[ix]\n",
    "    train_lgm = train_lgm[ix]\n",
    "\n",
    "\n",
    "    sample_model = TrainModel(train_x, train_y, train_lgm, val_x, val_y, val_lgm, en, title = title)\n",
    "\n",
    "    # code for ROC curve displays\n",
    "    if en == 0:\n",
    "        pred_ID_all = []\n",
    "        pred_LGm_all = []\n",
    "        true_ID_all = []\n",
    "        true_LGm_all = []\n",
    "\n",
    "    preds = sample_model.predict(val_x, batch_size=256)\n",
    "    pred_ID_all.append(preds[0])       # model output for ID\n",
    "    pred_LGm_all.append(preds[1])      # model output for LGm\n",
    "    true_ID_all.append(val_y)\n",
    "    true_LGm_all.append(val_lgm)\n",
    "\n",
    "\n",
    "    \n",
    "    # get performance on each sample\n",
    "    sample_evaluation(sample_model, train_x, train_y, train_lgm, val_x, val_y, val_lgm, en, title = title, verbose = False)\n",
    "\n",
    "\n",
    "    del train_x\n",
    "    del train_y\n",
    "    del val_x\n",
    "    del val_y\n",
    "    del sample_model\n",
    "    gc.collect()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ----- Compute ROC for both outputs -----\n",
    "fpr_ID, tpr_ID, auc_ID, classes_ID = compute_multiclass_roc(true_ID_all, pred_ID_all)\n",
    "fpr_LGm, tpr_LGm, auc_LGm, classes_LGm = compute_multiclass_roc(true_LGm_all, pred_LGm_all)\n",
    "\n",
    "# ---- ID head ----\n",
    "mean_fpr_ID, mean_tpr_ID, lower_tpr_ID, upper_tpr_ID, mean_auc_ID, auc_ci_ID = \\\n",
    "    plot_roc_with_folds_and_mean(true_ID_all, pred_ID_all, title=\"(\"+title+\")ROC_ID\", color='black')\n",
    "\n",
    "mean_recall_ID, mean_precision_ID, lower_precision_ID, upper_precision_ID, \\\n",
    "    mean_auprc_ID, auprc_ci_ID = \\\n",
    "    plot_pr_with_folds_and_mean(true_ID_all, pred_ID_all, \n",
    "                                title=\"(\"+title+\")PR_ID\", color='black')\n",
    "\n",
    "# ---- LGm head ----\n",
    "mean_fpr_LGm, mean_tpr_LGm, lower_tpr_LGm, upper_tpr_LGm, mean_auc_LGm, auc_ci_LGm = \\\n",
    "    plot_roc_with_folds_and_mean(true_LGm_all, pred_LGm_all, title=\"(\"+title+\")ROC_IDH1\", color='black')\n",
    "\n",
    "\n",
    "mean_recall_LGm, mean_precision_LGm, lower_precision_LGm, upper_precision_LGm, \\\n",
    "    mean_auprc_LGm, auprc_ci_LGm = \\\n",
    "    plot_pr_with_folds_and_mean(true_LGm_all, pred_LGm_all, \n",
    "                                title=\"(\"+title+\")PR_LGm\", color='black')\n",
    "\n",
    "\n",
    "# ---- Optionally save AUCs ----\n",
    "np.save(\"Results/\"+title+\"_AUC_ID.npy\", [mean_auc_ID, auc_ci_ID[0], auc_ci_ID[1]])\n",
    "np.save(\"Results/\"+title+\"_AUC_LGm.npy\", [mean_auc_LGm, auc_ci_LGm[0], auc_ci_LGm[1]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db9b41af-37da-46f1-87ec-76e446d14ccf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.preprocessing import label_binarize\n",
    "\n",
    "gc.collect()\n",
    "\n",
    "# Get and prepare the full dataset (join the static train-val-test splits for the full data)\n",
    "p = \"Data/\"\n",
    "title = \"RADAR\"\n",
    "X_all = np.concatenate([np.load(p + \"train_x_\"+title+\".npy\"), np.load(p + \"val_x_\"+title+\".npy\"), np.load(p + \"test_x_\"+title+\".npy\")])\n",
    "\n",
    "plt.figure(figsize = (8, 8))\n",
    "\n",
    "# Get the folds\n",
    "organizer = StratifiedKFold(n_splits=n_folds, shuffle = True, random_state = 42)\n",
    "folds = organizer.split(X_all, y_all)\n",
    "\n",
    "for en, fold in enumerate(folds):\n",
    "    train_ix = fold[0]\n",
    "    val_ix = fold[1]\n",
    "\n",
    "    # Training set\n",
    "    train_x = X_all[train_ix]\n",
    "    train_y = y_all[train_ix]\n",
    "    train_lgm = lgm_all[train_ix]\n",
    "\n",
    "    # Validation set\n",
    "    val_x = X_all[val_ix]\n",
    "    val_y = y_all[val_ix]\n",
    "    val_lgm = lgm_all[val_ix]\n",
    "\n",
    "    # Data shuffle\n",
    "    ix = np.arange(len(train_x))\n",
    "    np.random.shuffle(ix)\n",
    "    train_x = train_x[ix]\n",
    "    train_y = train_y[ix]\n",
    "    train_lgm = train_lgm[ix]\n",
    "\n",
    "    sample_model = TrainModel(train_x, train_y, train_lgm, val_x, val_y, val_lgm, en, title = title)\n",
    "\n",
    "    # code for ROC curve displays\n",
    "\n",
    "    if en == 0:\n",
    "        pred_ID_all = []\n",
    "        pred_LGm_all = []\n",
    "        true_ID_all = []\n",
    "        true_LGm_all = []\n",
    "\n",
    "    preds = sample_model.predict(val_x, batch_size=256)\n",
    "    pred_ID_all.append(preds[0])       # model output for ID\n",
    "    pred_LGm_all.append(preds[1])      # model output for LGm\n",
    "    true_ID_all.append(val_y)\n",
    "    true_LGm_all.append(val_lgm)\n",
    "\n",
    "    # get performance on each sample\n",
    "    sample_evaluation(sample_model, train_x, train_y, train_lgm, val_x, val_y, val_lgm, en, title = title, verbose = False)\n",
    "\n",
    "    del train_x\n",
    "    del train_y\n",
    "    del val_x\n",
    "    del val_y\n",
    "    del sample_model\n",
    "    gc.collect()\n",
    "\n",
    "# ----- Compute ROC for both outputs -----\n",
    "fpr_ID, tpr_ID, auc_ID, classes_ID = compute_multiclass_roc(true_ID_all, pred_ID_all)\n",
    "fpr_LGm, tpr_LGm, auc_LGm, classes_LGm = compute_multiclass_roc(true_LGm_all, pred_LGm_all)\n",
    "\n",
    "# ---- ID head ----\n",
    "mean_fpr_ID, mean_tpr_ID, lower_tpr_ID, upper_tpr_ID, mean_auc_ID, auc_ci_ID = \\\n",
    "    plot_roc_with_folds_and_mean(true_ID_all, pred_ID_all, title=\"(\"+title+\")ROC_ID\", color='black')\n",
    "\n",
    "# ---- LGm head ----\n",
    "mean_fpr_LGm, mean_tpr_LGm, lower_tpr_LGm, upper_tpr_LGm, mean_auc_LGm, auc_ci_LGm = \\\n",
    "    plot_roc_with_folds_and_mean(true_LGm_all, pred_LGm_all, title=\"(\"+title+\")ROC_IDH1\", color='black')\n",
    "\n",
    "# ---- Optionally save AUCs ----\n",
    "np.save(\"Results/\"+title+\"_AUC_ID.npy\", [mean_auc_ID, auc_ci_ID[0], auc_ci_ID[1]])\n",
    "np.save(\"Results/\"+title+\"_AUC_LGm.npy\", [mean_auc_LGm, auc_ci_LGm[0], auc_ci_LGm[1]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "656a9d4f-c75e-4eeb-bd45-2af3843836b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.preprocessing import label_binarize\n",
    "\n",
    "gc.collect()\n",
    "\n",
    "# Get and prepare the full dataset (join the static train-val-test splits for the full data)\n",
    "p = \"Data/\"\n",
    "title = \"RAW\"\n",
    "X_all = np.concatenate([np.load(p + \"train_x.npy\"), np.load(p + \"val_x.npy\"), np.load(p + \"test_x.npy\")])\n",
    "\n",
    "plt.figure(figsize = (8, 8))\n",
    "\n",
    "# Get the folds\n",
    "organizer = StratifiedKFold(n_splits=n_folds, shuffle = True, random_state = 42)\n",
    "folds = organizer.split(X_all, y_all)\n",
    "\n",
    "for en, fold in enumerate(folds):\n",
    "    train_ix = fold[0]\n",
    "    val_ix = fold[1]\n",
    "\n",
    "    # Training set\n",
    "    train_x = X_all[train_ix]\n",
    "    train_y = y_all[train_ix]\n",
    "    train_lgm = lgm_all[train_ix]\n",
    "\n",
    "    # Validation set\n",
    "    val_x = X_all[val_ix]\n",
    "    val_y = y_all[val_ix]\n",
    "    val_lgm = lgm_all[val_ix]\n",
    "\n",
    "    # Data shuffle\n",
    "    ix = np.arange(len(train_x))\n",
    "    np.random.shuffle(ix)\n",
    "    train_x = train_x[ix]\n",
    "    train_y = train_y[ix]\n",
    "    train_lgm = train_lgm[ix]\n",
    "\n",
    "    sample_model = TrainModel(train_x, train_y, train_lgm, val_x, val_y, val_lgm, en, title = title)\n",
    "\n",
    "    # code for ROC curve displays\n",
    "\n",
    "    if en == 0:\n",
    "        pred_ID_all = []\n",
    "        pred_LGm_all = []\n",
    "        true_ID_all = []\n",
    "        true_LGm_all = []\n",
    "\n",
    "    preds = sample_model.predict(val_x, batch_size=256)\n",
    "    pred_ID_all.append(preds[0])       # model output for ID\n",
    "    pred_LGm_all.append(preds[1])      # model output for LGm\n",
    "    true_ID_all.append(val_y)\n",
    "    true_LGm_all.append(val_lgm)\n",
    "\n",
    "    # get performance on each sample\n",
    "    sample_evaluation(sample_model, train_x, train_y, train_lgm, val_x, val_y, val_lgm, en, title = title, verbose = False)\n",
    "\n",
    "    del train_x\n",
    "    del train_y\n",
    "    del val_x\n",
    "    del val_y\n",
    "    del sample_model\n",
    "    gc.collect()\n",
    "\n",
    "# ----- Compute ROC for both outputs -----\n",
    "fpr_ID, tpr_ID, auc_ID, classes_ID = compute_multiclass_roc(true_ID_all, pred_ID_all)\n",
    "fpr_LGm, tpr_LGm, auc_LGm, classes_LGm = compute_multiclass_roc(true_LGm_all, pred_LGm_all)\n",
    "\n",
    "# ---- ID head ----\n",
    "mean_fpr_ID, mean_tpr_ID, lower_tpr_ID, upper_tpr_ID, mean_auc_ID, auc_ci_ID = \\\n",
    "    plot_roc_with_folds_and_mean(true_ID_all, pred_ID_all, title=\"(\"+title+\")ROC_ID\", color='black')\n",
    "\n",
    "# ---- LGm head ----\n",
    "mean_fpr_LGm, mean_tpr_LGm, lower_tpr_LGm, upper_tpr_LGm, mean_auc_LGm, auc_ci_LGm = \\\n",
    "    plot_roc_with_folds_and_mean(true_LGm_all, pred_LGm_all, title=\"(\"+title+\")ROC_IDH1\", color='black')\n",
    "\n",
    "# ---- Optionally save AUCs ----\n",
    "np.save(\"Results/\"+title+\"_AUC_ID.npy\", [mean_auc_ID, auc_ci_ID[0], auc_ci_ID[1]])\n",
    "np.save(\"Results/\"+title+\"_AUC_LGm.npy\", [mean_auc_LGm, auc_ci_LGm[0], auc_ci_LGm[1]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0c49f3a-0f63-41c3-96b5-4191c4a92bba",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.preprocessing import label_binarize\n",
    "\n",
    "gc.collect()\n",
    "\n",
    "# Get and prepare the full dataset (join the static train-val-test splits for the full data)\n",
    "p = \"Data/\"\n",
    "title = \"MANUAL\"\n",
    "X_all = np.concatenate([np.load(p + \"train_x_\"+title+\".npy\"), np.load(p + \"val_x_\"+title+\".npy\"), np.load(p + \"test_x_\"+title+\".npy\")])\n",
    "\n",
    "organizer = StratifiedKFold(n_splits=n_folds, shuffle = True, random_state = 42)\n",
    "folds = organizer.split(X_all, y_all)\n",
    "\n",
    "global_ID_acc = []\n",
    "global_IDH_acc = []\n",
    "\n",
    "sample_model = make_split_model(lr, out_dims = [len(np.unique(y_all)), len(np.unique(lgm_all))],\n",
    "                                losses = [\"sparse_categorical_crossentropy\", \"sparse_categorical_crossentropy\"])\n",
    "sample_model.summary()\n",
    "for en, fold in enumerate(folds):\n",
    "    train_ix = fold[0]\n",
    "    val_ix = fold[1]\n",
    "\n",
    "    # Training set\n",
    "    train_x = X_all[train_ix]\n",
    "    train_y = y_all[train_ix]\n",
    "    train_lgm = lgm_all[train_ix]\n",
    "\n",
    "    # Validation set\n",
    "    val_x = X_all[val_ix]\n",
    "    val_y = y_all[val_ix]\n",
    "    val_lgm = lgm_all[val_ix]\n",
    "\n",
    "    # Data shuffle\n",
    "    ix = np.arange(len(train_x))\n",
    "    np.random.shuffle(ix)\n",
    "    train_x = train_x[ix]\n",
    "    train_y = train_y[ix]\n",
    "    train_lgm = train_lgm[ix]\n",
    "\n",
    "    sample_model.load_weights(\"Models/FullTest_\"+title+\"_Model_\"+str(en)+\".h5\")\n",
    "\n",
    "    preds = sample_model.predict(val_x)\n",
    "    id_preds = np.argmax(preds[0], axis = 1)\n",
    "    lgm_preds = np.argmax(preds[1], axis = 1)\n",
    "\n",
    "    global_ID_acc.append(balanced_accuracy_score(val_y, id_preds))\n",
    "    global_IDH_acc.append(balanced_accuracy_score(val_y, lgm_preds))\n",
    "    print(id_preds)\n",
    "    print(lgm_preds)\n",
    "\n",
    "    # code for ROC curve displays\n",
    "    if en == 0:\n",
    "        pred_ID_all = []\n",
    "        pred_LGm_all = []\n",
    "        true_ID_all = []\n",
    "        true_LGm_all = []\n",
    "\n",
    "    pred_ID_all.append(preds[0])       # model output for ID\n",
    "    pred_LGm_all.append(preds[1])      # model output for LGm\n",
    "    true_ID_all.append(val_y)\n",
    "    true_LGm_all.append(val_lgm)\n",
    "    \n",
    "    # get performance on each sample\n",
    "    # sample_evaluation(sample_model, train_x, train_y, train_lgm, val_x, val_y, val_lgm, en, title = title, verbose = False)\n",
    "    del train_x\n",
    "    del train_y\n",
    "    del val_x\n",
    "    del val_y\n",
    "    \n",
    "    gc.collect()\n",
    "\n",
    "\n",
    "print(global_ID_acc)\n",
    "print(global_IDH_acc)\n",
    "print(np.mean(global_ID_acc), np.mean(global_IDH_acc))\n",
    "\n",
    "# ----- Compute ROC for both outputs -----\n",
    "fpr_ID, tpr_ID, auc_ID, classes_ID = compute_multiclass_roc(true_ID_all, pred_ID_all)\n",
    "fpr_LGm, tpr_LGm, auc_LGm, classes_LGm = compute_multiclass_roc(true_LGm_all, pred_LGm_all)\n",
    "\n",
    "# ---- ID head ----\n",
    "mean_fpr_ID, mean_tpr_ID, lower_tpr_ID, upper_tpr_ID, mean_auc_ID, auc_ci_ID = \\\n",
    "    plot_roc_with_folds_and_mean(true_ID_all, pred_ID_all, title=\"(\"+title+\")AUC_ID\", color='black')\n",
    "\n",
    "mean_fpr_ID, mean_tpr_ID, lower_tpr_ID, upper_tpr_ID, mean_auc_ID, auc_ci_ID = \\\n",
    "    plot_pr_with_folds_and_mean(true_ID_all, pred_ID_all, title=\"(\"+title+\")PR_ID\", color='black')\n",
    "\n",
    "# ---- LGm head ----\n",
    "mean_fpr_LGm, mean_tpr_LGm, lower_tpr_LGm, upper_tpr_LGm, mean_auc_LGm, auc_ci_LGm = \\\n",
    "    plot_roc_with_folds_and_mean(true_LGm_all, pred_LGm_all, title=\"(\"+title+\")AUC_IDH1\", color='black')\n",
    "\n",
    "mean_fpr_LGm, mean_tpr_LGm, lower_tpr_LGm, upper_tpr_LGm, mean_auc_LGm, auc_ci_LGm = \\\n",
    "    plot_pr_with_folds_and_mean(true_LGm_all, pred_LGm_all, title=\"(\"+title+\")PR_IDH1\", color='black')\n",
    "\n",
    "# ---- Optionally save AUCs ----\n",
    "np.save(\"Results/\"+title+\"_AUC_ID.npy\", [mean_auc_ID, auc_ci_ID[0], auc_ci_ID[1]])\n",
    "np.save(\"Results/\"+title+\"_AUC_LGm.npy\", [mean_auc_LGm, auc_ci_LGm[0], auc_ci_LGm[1]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edf98603-2c83-4953-a825-50abbc1912d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.preprocessing import label_binarize\n",
    "\n",
    "gc.collect()\n",
    "\n",
    "# Get and prepare the full dataset (join the static train-val-test splits for the full data)\n",
    "p = \"Data/\"\n",
    "title = \"RADAR\"\n",
    "X_all = np.concatenate([np.load(p + \"train_x_\"+title+\".npy\"), np.load(p + \"val_x_\"+title+\".npy\"), np.load(p + \"test_x_\"+title+\".npy\")])\n",
    "\n",
    "organizer = StratifiedKFold(n_splits=n_folds, shuffle = True, random_state = 42)\n",
    "folds = organizer.split(X_all, y_all)\n",
    "\n",
    "sample_model = make_split_model(lr, out_dims = [len(np.unique(y_all)), len(np.unique(lgm_all))],\n",
    "                                losses = [\"sparse_categorical_crossentropy\", \"sparse_categorical_crossentropy\"])\n",
    "sample_model.summary()\n",
    "global_ID_acc = []\n",
    "global_IDH_acc = []\n",
    "\n",
    "for en, fold in enumerate(folds):\n",
    "    train_ix = fold[0]\n",
    "    val_ix = fold[1]\n",
    "\n",
    "    # Training set\n",
    "    train_x = X_all[train_ix]\n",
    "    train_y = y_all[train_ix]\n",
    "    train_lgm = lgm_all[train_ix]\n",
    "\n",
    "    # Validation set\n",
    "    val_x = X_all[val_ix]\n",
    "    val_y = y_all[val_ix]\n",
    "    val_lgm = lgm_all[val_ix]\n",
    "\n",
    "    # Data shuffle\n",
    "    ix = np.arange(len(train_x))\n",
    "    np.random.shuffle(ix)\n",
    "    train_x = train_x[ix]\n",
    "    train_y = train_y[ix]\n",
    "    train_lgm = train_lgm[ix]\n",
    "\n",
    "\n",
    "\n",
    "    sample_model.load_weights(\"Models/FullTest_\"+title+\"_Model_\"+str(en)+\".h5\")\n",
    "\n",
    "    preds = sample_model.predict(val_x)\n",
    "    id_preds = np.argmax(preds[0], axis = 1)\n",
    "    lgm_preds = np.argmax(preds[1], axis = 1)\n",
    "\n",
    "    global_ID_acc.append(balanced_accuracy_score(val_y, id_preds))\n",
    "    global_IDH_acc.append(balanced_accuracy_score(val_y, lgm_preds))\n",
    "    print(id_preds)\n",
    "    print(lgm_preds)\n",
    "\n",
    "    # code for ROC curve displays\n",
    "    if en == 0:\n",
    "        pred_ID_all = []\n",
    "        pred_LGm_all = []\n",
    "        true_ID_all = []\n",
    "        true_LGm_all = []\n",
    "\n",
    "    pred_ID_all.append(preds[0])       # model output for ID\n",
    "    pred_LGm_all.append(preds[1])      # model output for LGm\n",
    "    true_ID_all.append(val_y)\n",
    "    true_LGm_all.append(val_lgm)\n",
    "\n",
    "\n",
    "    \n",
    "    # get performance on each sample\n",
    "    # sample_evaluation(sample_model, train_x, train_y, train_lgm, val_x, val_y, val_lgm, en, title = title, verbose = False)\n",
    "    del train_x\n",
    "    del train_y\n",
    "    del val_x\n",
    "    del val_y\n",
    "    gc.collect()\n",
    "\n",
    "\n",
    "print(global_ID_acc)\n",
    "print(global_IDH_acc)\n",
    "print(np.mean(global_ID_acc), np.mean(global_IDH_acc))\n",
    "\n",
    "# ----- Compute ROC for both outputs -----\n",
    "fpr_ID, tpr_ID, auc_ID, classes_ID = compute_multiclass_roc(true_ID_all, pred_ID_all)\n",
    "fpr_LGm, tpr_LGm, auc_LGm, classes_LGm = compute_multiclass_roc(true_LGm_all, pred_LGm_all)\n",
    "\n",
    "# ---- ID head ----\n",
    "mean_fpr_ID, mean_tpr_ID, lower_tpr_ID, upper_tpr_ID, mean_auc_ID, auc_ci_ID = \\\n",
    "    plot_roc_with_folds_and_mean(true_ID_all, pred_ID_all, title=\"(\"+title+\")AUC_ID\", color='black')\n",
    "\n",
    "mean_fpr_ID, mean_tpr_ID, lower_tpr_ID, upper_tpr_ID, mean_auc_ID, auc_ci_ID = \\\n",
    "    plot_pr_with_folds_and_mean(true_ID_all, pred_ID_all, title=\"(\"+title+\")PR_ID\", color='black')\n",
    "\n",
    "# ---- LGm head ----\n",
    "mean_fpr_LGm, mean_tpr_LGm, lower_tpr_LGm, upper_tpr_LGm, mean_auc_LGm, auc_ci_LGm = \\\n",
    "    plot_roc_with_folds_and_mean(true_LGm_all, pred_LGm_all, title=\"(\"+title+\")AUC_IDH1\", color='black')\n",
    "\n",
    "mean_fpr_LGm, mean_tpr_LGm, lower_tpr_LGm, upper_tpr_LGm, mean_auc_LGm, auc_ci_LGm = \\\n",
    "    plot_pr_with_folds_and_mean(true_LGm_all, pred_LGm_all, title=\"(\"+title+\")PR_IDH1\", color='black')\n",
    "\n",
    "# ---- Optionally save AUCs ----\n",
    "np.save(\"Results/\"+title+\"_AUC_ID.npy\", [mean_auc_ID, auc_ci_ID[0], auc_ci_ID[1]])\n",
    "np.save(\"Results/\"+title+\"_AUC_LGm.npy\", [mean_auc_LGm, auc_ci_LGm[0], auc_ci_LGm[1]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "911abdf9-6330-41f4-a430-7ec9d2903051",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.preprocessing import label_binarize\n",
    "\n",
    "gc.collect()\n",
    "\n",
    "# Get and prepare the full dataset (join the static train-val-test splits for the full data)\n",
    "p = \"Data/\"\n",
    "title = \"RAW\"\n",
    "X_all = np.concatenate([np.load(p + \"train_x.npy\"), np.load(p + \"val_x.npy\"), np.load(p + \"test_x.npy\")])\n",
    "\n",
    "\n",
    "organizer = StratifiedKFold(n_splits=n_folds, shuffle = True, random_state = 42)\n",
    "folds = organizer.split(X_all, y_all)\n",
    "\n",
    "sample_model = make_split_model(lr, out_dims = [len(np.unique(y_all)), len(np.unique(lgm_all))],\n",
    "                                losses = [\"sparse_categorical_crossentropy\", \"sparse_categorical_crossentropy\"])\n",
    "\n",
    "global_ID_acc = []\n",
    "global_IDH_acc = []\n",
    "\n",
    "for en, fold in enumerate(folds):\n",
    "    train_ix = fold[0]\n",
    "    val_ix = fold[1]\n",
    "\n",
    "    # Training set\n",
    "    train_x = X_all[train_ix]\n",
    "    train_y = y_all[train_ix]\n",
    "    train_lgm = lgm_all[train_ix]\n",
    "\n",
    "    # Validation set\n",
    "    val_x = X_all[val_ix]\n",
    "    val_y = y_all[val_ix]\n",
    "    val_lgm = lgm_all[val_ix]\n",
    "\n",
    "    # Data shuffle\n",
    "    ix = np.arange(len(train_x))\n",
    "    np.random.shuffle(ix)\n",
    "    train_x = train_x[ix]\n",
    "    train_y = train_y[ix]\n",
    "    train_lgm = train_lgm[ix]\n",
    "\n",
    "    sample_model.load_weights(\"Models/FullTest_\"+title+\"_Model_\"+str(en)+\".h5\")\n",
    "\n",
    "    preds = sample_model.predict(val_x)\n",
    "    id_preds = np.argmax(preds[0], axis = 1)\n",
    "    lgm_preds = np.argmax(preds[1], axis = 1)\n",
    "\n",
    "    global_ID_acc.append(balanced_accuracy_score(val_y, id_preds))\n",
    "    global_IDH_acc.append(balanced_accuracy_score(val_y, lgm_preds))\n",
    "    print(id_preds)\n",
    "    print(lgm_preds)\n",
    "\n",
    "    # code for ROC curve displays\n",
    "    if en == 0:\n",
    "        pred_ID_all = []\n",
    "        pred_LGm_all = []\n",
    "        true_ID_all = []\n",
    "        true_LGm_all = []\n",
    "\n",
    "    pred_ID_all.append(preds[0])       # model output for ID\n",
    "    pred_LGm_all.append(preds[1])      # model output for LGm\n",
    "    true_ID_all.append(val_y)\n",
    "    true_LGm_all.append(val_lgm)\n",
    "\n",
    "\n",
    "    \n",
    "    # get performance on each sample\n",
    "    # sample_evaluation(sample_model, train_x, train_y, train_lgm, val_x, val_y, val_lgm, en, title = title, verbose = False)\n",
    "    del train_x\n",
    "    del train_y\n",
    "    del val_x\n",
    "    del val_y\n",
    "    gc.collect()\n",
    "\n",
    "\n",
    "print(global_ID_acc)\n",
    "print(global_IDH_acc)\n",
    "print(np.mean(global_ID_acc), np.mean(global_IDH_acc))\n",
    "\n",
    "# ----- Compute ROC for both outputs -----\n",
    "fpr_ID, tpr_ID, auc_ID, classes_ID = compute_multiclass_roc(true_ID_all, pred_ID_all)\n",
    "fpr_LGm, tpr_LGm, auc_LGm, classes_LGm = compute_multiclass_roc(true_LGm_all, pred_LGm_all)\n",
    "\n",
    "# ---- ID head ----\n",
    "mean_fpr_ID, mean_tpr_ID, lower_tpr_ID, upper_tpr_ID, mean_auc_ID, auc_ci_ID = \\\n",
    "    plot_roc_with_folds_and_mean(true_ID_all, pred_ID_all, title=\"(\"+title+\")AUC_ID\", color='black')\n",
    "\n",
    "mean_fpr_ID, mean_tpr_ID, lower_tpr_ID, upper_tpr_ID, mean_auc_ID, auc_ci_ID = \\\n",
    "    plot_pr_with_folds_and_mean(true_ID_all, pred_ID_all, title=\"(\"+title+\")PR_ID\", color='black')\n",
    "\n",
    "# ---- LGm head ----\n",
    "mean_fpr_LGm, mean_tpr_LGm, lower_tpr_LGm, upper_tpr_LGm, mean_auc_LGm, auc_ci_LGm = \\\n",
    "    plot_roc_with_folds_and_mean(true_LGm_all, pred_LGm_all, title=\"(\"+title+\")AUC_IDH1\", color='black')\n",
    "\n",
    "mean_fpr_LGm, mean_tpr_LGm, lower_tpr_LGm, upper_tpr_LGm, mean_auc_LGm, auc_ci_LGm = \\\n",
    "    plot_pr_with_folds_and_mean(true_LGm_all, pred_LGm_all, title=\"(\"+title+\")PR_IDH1\", color='black')\n",
    "\n",
    "# ---- Optionally save AUCs ----\n",
    "np.save(\"Results/\"+title+\"_AUC_ID.npy\", [mean_auc_ID, auc_ci_ID[0], auc_ci_ID[1]])\n",
    "np.save(\"Results/\"+title+\"_AUC_LGm.npy\", [mean_auc_LGm, auc_ci_LGm[0], auc_ci_LGm[1]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c0b9f03-28a5-43e6-8e3a-67da68db8403",
   "metadata": {},
   "outputs": [],
   "source": [
    "id_dict = {}\n",
    "lgm_dict = {}\n",
    "\n",
    "for title in [\"MANUAL\", \"RADAR\", \"RAW\"]:\n",
    "\n",
    "    accs = []\n",
    "    for en in range(n_folds):\n",
    "        #hist = np.load(\"Results/FullTest_\"+title+\"_hist_\"+str(en)+\".npy\", allow_pickle = True)\n",
    "        acc = np.load(\"Results/\"+title+\"all_accuracies_\"+str(en)+\".npy\", allow_pickle = True)\n",
    "        accs.append(acc[:, -3:-1])\n",
    "\n",
    "    accs = np.array(accs)\n",
    "\n",
    "    mean = np.mean(accs, axis = 0)\n",
    "    id_dict[title] = mean.T[0]\n",
    "    lgm_dict[title] = mean.T[1]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa8be85c-b15f-4276-a178-11357a05241a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.stats as stats\n",
    "################\n",
    "# ID accuracies\n",
    "################\n",
    "\n",
    "sorting = np.argsort(id_dict[\"RAW\"])\n",
    "x_range = np.arange(len(id_dict[\"RAW\"]))\n",
    "\n",
    "plt.figure(figsize = (10, 5))\n",
    "\n",
    "\n",
    "conf_level = 0.95\n",
    "d = id_dict[\"RAW\"]\n",
    "RAW_cl = stats.t.interval(conf_level, df=len(d)-1, loc=np.mean(d), scale=np.std(d, ddof=1) / np.sqrt(len(d)))\n",
    "RAW_delta = np.round(np.max(np.abs(np.mean(d) - RAW_cl)), 3)\n",
    "d = id_dict[\"MANUAL\"]\n",
    "MANUAL_cl = stats.t.interval(conf_level, df=len(d)-1, loc=np.mean(d), scale=np.std(d, ddof=1) / np.sqrt(len(d)))\n",
    "MANUAL_delta = np.round(np.max(np.abs(np.mean(d) - MANUAL_cl)), 3)\n",
    "d = id_dict[\"RADAR\"]\n",
    "RADAR_cl = stats.t.interval(conf_level, df=len(d)-1, loc=np.mean(d), scale=np.std(d, ddof=1) / np.sqrt(len(d)))\n",
    "RADAR_delta = np.round(np.max(np.abs(np.mean(d) - RADAR_cl)), 3)\n",
    "\n",
    "\n",
    "plt.scatter(x_range, id_dict[\"RAW\"][sorting], label = \"Raw data:\" + str(np.round(np.mean(id_dict[\"RAW\"]), 2)) + \", CI: ± \" + str(RAW_delta))\n",
    "plt.scatter(x_range, id_dict[\"MANUAL\"][sorting], label = \"Manual data:\" + str(np.round(np.mean(id_dict[\"MANUAL\"]), 2)) + \", CI: ± \" + str(MANUAL_delta))\n",
    "plt.scatter(x_range, id_dict[\"RADAR\"][sorting], label = \"RADAR data:\" + str(np.round(np.mean(id_dict[\"RADAR\"]), 2)) + \", CI: ± \" + str(RADAR_delta))\n",
    "\n",
    "plt.ylim([-0.1, 1.1])\n",
    "plt.legend(fontsize = 25, loc = \"upper left\")\n",
    "\n",
    "plt.savefig(\"Images/Histories/ID_ACC_comparison.png\", format=\"png\", transparent = True,\n",
    "                    dpi = 1000,\n",
    "                    bbox_inches='tight',\n",
    "                    pad_inches=0.5)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "################\n",
    "# LGm accuracies\n",
    "################\n",
    "\n",
    "sorting = np.argsort(lgm_dict[\"RAW\"])\n",
    "\n",
    "\n",
    "d = lgm_dict[\"RAW\"]\n",
    "RAW_cl = stats.t.interval(conf_level, df=len(d)-1, loc=np.mean(d), scale=np.std(d, ddof=1) / np.sqrt(len(d)))\n",
    "RAW_delta = np.round(np.max(np.abs(np.mean(d) - RAW_cl)), 3)\n",
    "d = lgm_dict[\"MANUAL\"]\n",
    "MANUAL_cl = stats.t.interval(conf_level, df=len(d)-1, loc=np.mean(d), scale=np.std(d, ddof=1) / np.sqrt(len(d)))\n",
    "MANUAL_delta = np.round(np.max(np.abs(np.mean(d) - MANUAL_cl)), 3)\n",
    "d = lgm_dict[\"RADAR\"]\n",
    "RADAR_cl = stats.t.interval(conf_level, df=len(d)-1, loc=np.mean(d), scale=np.std(d, ddof=1) / np.sqrt(len(d)))\n",
    "RADAR_delta = np.round(np.max(np.abs(np.mean(d) - RADAR_cl)), 3)\n",
    "\n",
    "\n",
    "plt.figure(figsize = (10, 5))\n",
    "\n",
    "plt.scatter(x_range, lgm_dict[\"RAW\"][sorting], label = \"Raw data:\" + str(np.round(np.mean(lgm_dict[\"RAW\"]), 2)) + \", CI: ± \" + str(RAW_delta))\n",
    "plt.scatter(x_range, lgm_dict[\"MANUAL\"][sorting], label = \"Manual data:\" + str(np.round(np.mean(lgm_dict[\"MANUAL\"]), 2)) + \", CI: ± \" + str(MANUAL_delta))\n",
    "plt.scatter(x_range, lgm_dict[\"RADAR\"][sorting], label = \"RADAR data:\" + str(np.round(np.mean(lgm_dict[\"RADAR\"]), 2)) + \", CI: ± \" + str(RADAR_delta))\n",
    "\n",
    "plt.ylim([-0.1, 1.1])\n",
    "plt.legend(fontsize = 25, loc = \"lower right\")\n",
    "\n",
    "plt.savefig(\"Images/Histories/LGM_ACC_comparison.png\", format=\"png\", transparent = True,\n",
    "                    dpi = 1000,\n",
    "                    bbox_inches='tight',\n",
    "                    pad_inches=0.5)\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "796466b7-2696-417a-9f32-50cf5448f556",
   "metadata": {},
   "source": [
    "# Adding the reduction script here to continue running"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "614bacd1-a93a-4b53-ab2b-128095d6c41b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from Scripts.essentials import *\n",
    "from matplotlib.animation import FuncAnimation\n",
    "import io\n",
    "from PIL import Image\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.metrics import precision_recall_curve, average_precision_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87c63403-2a12-4da2-9eb4-6e95a6f7a4c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "title = \"MANUAL\"\n",
    "\n",
    "X_all = np.concatenate([np.load(p + \"train_x_\"+title+\".npy\"), np.load(p + \"val_x_\"+title+\".npy\"), np.load(p + \"test_x_\"+title+\".npy\")])\n",
    "lgm_all = np.concatenate([np.load(p + \"train_lgm.npy\"), np.load(p + \"val_lgm.npy\"), np.load(p + \"test_lgm.npy\")])\n",
    "y_all = np.concatenate([np.load(p + \"train_y_46.npy\"), np.load(p + \"val_y_46.npy\"), np.load(p + \"test_y_46.npy\")])\n",
    "lgm_all = np.argmax(lgm_all, axis = -1)\n",
    "y_all = np.argmax(y_all, axis = -1)\n",
    "lgm_all = np.where(lgm_all > 2, 0, 1)\n",
    "\n",
    "# Get the folds\n",
    "organizer = StratifiedKFold(n_splits=n_folds, shuffle = True, random_state = 42)\n",
    "folds = organizer.split(X_all, y_all)\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aec0590e-6326-4d60-988d-11d1093aa29f",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_lr = 0.001\n",
    "decay_rate = 0.05\n",
    "\n",
    "\n",
    "# Create animation of feature importance vector evolution\n",
    "def plotImp(i, histories):\n",
    "    plt.clf()\n",
    "    fig, ax = plt.subplots(2, figsize = (20, 10))\n",
    "    \n",
    "    \n",
    "    mean = np.mean(train_x, axis = 0) * importances[i]\n",
    "    \n",
    "        #ax[0].plot(mean, color = \"Red\", alpha = 0.5)\n",
    "    ax[0].plot(importances[i], alpha =  0.1, linestyle = \"--\", color = \"Blue\")\n",
    "                 \n",
    "    ax[0].scatter(np.arange(1738), importances[i], color = \"blue\", alpha = 0.2, s = 5) # Scatter all features\n",
    "    ax[0].set_ylim([0, 1.05])\n",
    "        \n",
    "    ax[1].fill_between(np.arange(len(transformations[i])), all_transformations[i][0], all_transformations[i][1])\n",
    "    ax[1].plot(transformations[i], color = \"red\", alpha = 0.5)\n",
    "    ax[1].set_ylim([0, 1.05])\n",
    "        \n",
    "    plt.title(\"Epoch: \" + str(i) + \"    Val ID Acc: \" + str(np.round(histories[i][0], 2)) + \"    Val LGm Acc: \" + str(np.round(histories[i][1], 2)))\n",
    "                \n",
    "    \n",
    "plt.figure(figsize = (8, 8))\n",
    "for en, fold in enumerate(folds):\n",
    "    \n",
    "    lr = start_lr\n",
    "    \n",
    "    train_ix = fold[0]\n",
    "    val_ix = fold[1]\n",
    "\n",
    "    # Training set\n",
    "    train_x = X_all[train_ix]\n",
    "    train_y = y_all[train_ix]\n",
    "    train_lgm = lgm_all[train_ix]\n",
    "\n",
    "    # Validation set\n",
    "    val_x = X_all[val_ix]\n",
    "    val_y = y_all[val_ix]\n",
    "    val_lgm = lgm_all[val_ix]\n",
    "\n",
    "    # Data shuffle\n",
    "    ix = np.arange(len(train_x))\n",
    "    np.random.shuffle(ix)\n",
    "    train_x = train_x[ix]\n",
    "    train_y = train_y[ix]\n",
    "    train_lgm = train_lgm[ix]\n",
    "\n",
    "    \n",
    "\n",
    "    hist_I = []\n",
    "    hist_II = []\n",
    "\n",
    "    # Compute the weights for each category type\n",
    "    counts = np.bincount(train_y)\n",
    "    class_weights = np.sqrt((1/(counts/np.max(counts))))\n",
    "    \n",
    "    cw_id = {}\n",
    "    \n",
    "    for i in range(len(class_weights)):\n",
    "        cw_id[i] = class_weights[i]\n",
    "        \n",
    "    counts = np.bincount(train_lgm)\n",
    "    class_weights = np.sqrt((1/(counts/np.max(counts))))\n",
    "    \n",
    "    cw_lgm = {}\n",
    "    \n",
    "    for i in range(len(class_weights)):\n",
    "        cw_lgm[i] = class_weights[i]\n",
    "    \n",
    "    # Get sample wise weights to enable multi-output-balancing\n",
    "    sw_id = np.array([cw_id[c] for c in train_y])\n",
    "    sw_lgm = np.array([cw_lgm[c] for c in train_lgm])\n",
    "\n",
    "    reset_seed()\n",
    "    enc = make_encoder(feature_max_increment = (len(train_x)/batch_size) * epochs)\n",
    "    bias_model = make_split_model(out_dims = [len(np.unique(train_y)), 2],\n",
    "                                         losses = [\"sparse_categorical_crossentropy\", \"sparse_categorical_crossentropy\"])\n",
    "    \n",
    "    importances = []\n",
    "    imp = enc.get_layer(\"importance\").importance.numpy()\n",
    "    importances.append(imp)\n",
    "    \n",
    "    transformations = []\n",
    "    \n",
    "    transf = np.squeeze(enc.predict(np.expand_dims(np.mean(train_x, axis = 0), 0)))\n",
    "    transformations.append(transf)\n",
    "    \n",
    "    \n",
    "    fig, ax = plt.subplots(1, 2, figsize = (15, 7))\n",
    "    ax[0].plot(imp)\n",
    "    ax[0].set_ylim([0, 1])\n",
    "        \n",
    "    ax[1].plot(transf)\n",
    "    \n",
    "    \n",
    "    del transf\n",
    "    gc.collect()\n",
    "    \n",
    "    all_transformations = []\n",
    "    \n",
    "    transf = np.squeeze(enc.predict(np.expand_dims(train_x[:1024], 0)))\n",
    "    std = np.std(transf, axis = 0)\n",
    "    mean = np.mean(transf, axis = 0)\n",
    "    all_transformations.append([mean - std, mean + std])\n",
    "    \n",
    "    ax[1].fill_between(np.arange(1738), mean - std, mean + std, alpha = 0.5, color = \"black\")\n",
    "    ax[1].set_ylim([0, 1])\n",
    "     \n",
    "    plt.show()\n",
    "    \n",
    "    del transf, mean, std, imp\n",
    "    \n",
    "    gc.collect()\n",
    "    histories = []\n",
    "    \n",
    "    \n",
    "    for repeat in range(epochs):\n",
    "        \n",
    "        print(\"Repeat:\", str(repeat+1), \", alpha:\", str(lr))\n",
    "    \n",
    "        #################\n",
    "        # I\n",
    "        #################\n",
    "        \"\"\"\n",
    "        Train the model with a locked feature importance layer positioned immediately after the input\n",
    "        \"\"\"\n",
    "        enc.trainable = False\n",
    "        bias_model.trainable = True\n",
    "        reset_seed()\n",
    "\n",
    "        \n",
    "        split_model = make_combined_model(enc, bias_model,\n",
    "                                             lr = lr,\n",
    "                                             losses = [\"sparse_categorical_crossentropy\", \"sparse_categorical_crossentropy\"])\n",
    "        \n",
    "        print(\"Train the id and MutWt models\")\n",
    "        hist_I.append(split_model.fit(train_x,\n",
    "                         [train_y, train_lgm],                # list instead of dict\n",
    "                        sample_weight=[sw_id, sw_lgm],\n",
    "                        batch_size = batch_size,\n",
    "                        epochs = 1,\n",
    "                       validation_data=(val_x, [val_y, val_lgm])\n",
    "                       ).history\n",
    "        )\n",
    "        \n",
    "        # II\n",
    "        \"\"\"\n",
    "        Train the Feature importance layer by unlocking it. Lock the other layers to prevent the model from immediately adjusting to the \n",
    "        learned feature transformations.\n",
    "        \"\"\"\n",
    "        enc.trainable = True\n",
    "        bias_model.trainable = False\n",
    "        reset_seed()\n",
    "        split_model = make_combined_model(enc, bias_model,\n",
    "                                             lr = lr,\n",
    "                                             losses = [negative_CE, \"sparse_categorical_crossentropy\"],\n",
    "                                             metrics = [\"accuracy\"])\n",
    "        \n",
    "        print(\"Train the encoder to decrease id accuracy and maintain MutWt accuracy\")\n",
    "        hist_II.append(split_model.fit(train_x, \n",
    "                     [train_y, train_lgm],                # list instead of dict\n",
    "                        sample_weight=[sw_id, sw_lgm],\n",
    "                    batch_size = batch_size,\n",
    "                    epochs = 1,\n",
    "                    validation_data=(val_x, [val_y, val_lgm]) # Provide true patient ids to the validation to see how accuracy decreases on unseen data\n",
    "                    ).history\n",
    "                      )\n",
    "    \n",
    "        # Gather metrics and signals for plotting the gif later\n",
    "        imp = enc.get_layer(\"importance\").importance.numpy()\n",
    "        importances.append(imp)\n",
    "    \n",
    "        transf = np.squeeze(enc.predict(np.expand_dims(np.mean(train_x, axis = 0), 0)))\n",
    "        transformations.append(transf)\n",
    "    \n",
    "        fig, ax = plt.subplots(1, 2, figsize = (15, 7))\n",
    "        ax[0].plot(imp)\n",
    "        ax[0].set_ylim([0, 1])\n",
    "        \n",
    "        p = split_model.predict(val_x)\n",
    "        y_1 = np.argmax(p[0], axis = 1)\n",
    "        y_2 = np.argmax(p[1], axis = 1)\n",
    "        \n",
    "        h1 = balanced_accuracy_score(val_y, y_1)\n",
    "        h2 = balanced_accuracy_score(val_lgm, y_2)\n",
    "        \n",
    "        histories.append([h1, h2])\n",
    "        ax[1].plot(transf)\n",
    "    \n",
    "        del transf\n",
    "    \n",
    "        transf = np.squeeze(enc.predict(np.expand_dims(train_x[:1024], 0)))\n",
    "        std = np.std(transf, axis = 0)\n",
    "        mean = np.mean(transf, axis = 0)\n",
    "        all_transformations.append([mean - std, mean + std])\n",
    "    \n",
    "        ax[1].fill_between(np.arange(1738), mean - std, mean + std, alpha = 0.5, color = \"black\")\n",
    "        ax[1].set_ylim([0, 1])\n",
    "        \n",
    "        \n",
    "        plt.show()\n",
    "        \n",
    "        del split_model\n",
    "        del transf, mean, std, imp\n",
    "        gc.collect()\n",
    "    \n",
    "        lr = lr - (lr * decay_rate)\n",
    "\n",
    "        np.save(\"Results/Features/(MANUAL)MutantVsWildtype_importance\"+str(en)+\".npy\", enc.get_layer(\"importance\").importance.numpy())\n",
    "        np.save(\"Results/Features/(MANUAL)MutantVsWildtype_maximum\"+str(en)+\".npy\", enc.get_layer(\"importance\").maximum.numpy())\n",
    "        enc.save_weights(\"Models/data_encoders/(MANUAL)MutantVsWildtype_importance\"+str(en)+\".h5\")\n",
    "\n",
    "\n",
    "    plt.rcParams.update({'font.size': 20})\n",
    "    spec = train_x[0]\n",
    "    \n",
    "\n",
    "    fig = plt.figure(figsize=(14, 7))\n",
    "    frames = []\n",
    "    for i in range(len(importances)-1):\n",
    "        plotImp(i, histories)\n",
    "        buf = io.BytesIO()\n",
    "        plt.savefig(buf, format=\"png\")\n",
    "        plt.close()\n",
    "        buf.seek(0)\n",
    "        frames.append(Image.open(buf))\n",
    "\n",
    "    # Create and save the animated GIF\n",
    "    frames[0].save(\n",
    "        \"(MANUAL)FeatureImportanceEvolution_\"+str(en)+\".gif\",\n",
    "        save_all=True,\n",
    "        append_images=frames[1:],\n",
    "        duration=100,\n",
    "        loop=0,\n",
    "    )\n",
    "    del frames, fig, spec\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18597954-ade1-478e-a349-655c03491b98",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 1e-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15d76deb-08ca-4660-9486-e2cfe222e843",
   "metadata": {},
   "outputs": [],
   "source": [
    "p = \"Data/\"\n",
    "title = \"MANUAL\"\n",
    "\n",
    "X_all = np.concatenate([np.load(p + \"train_x_\"+title+\".npy\"), np.load(p + \"val_x_\"+title+\".npy\"), np.load(p + \"test_x_\"+title+\".npy\")])\n",
    "lgm_all = np.concatenate([np.load(p + \"train_lgm.npy\"), np.load(p + \"val_lgm.npy\"), np.load(p + \"test_lgm.npy\")])\n",
    "y_all = np.concatenate([np.load(p + \"train_y_46.npy\"), np.load(p + \"val_y_46.npy\"), np.load(p + \"test_y_46.npy\")])\n",
    "lgm_all = np.argmax(lgm_all, axis = -1)\n",
    "y_all = np.argmax(y_all, axis = -1)\n",
    "lgm_all = np.where(lgm_all > 2, 0, 1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7edfd0cc-f00d-4c33-8def-a9a2de14c182",
   "metadata": {},
   "outputs": [],
   "source": [
    "def TrainModel(train_x, train_y, train_lgm, val_x, val_y, val_lgm, en, title):\n",
    "    \n",
    "    # Compute the weights for each category type\n",
    "    counts = np.bincount(train_y)\n",
    "    class_weights = np.sqrt((1/(counts/np.max(counts))))\n",
    "    \n",
    "    cw_id = {}\n",
    "    \n",
    "    for i in range(len(class_weights)):\n",
    "        cw_id[i] = class_weights[i]\n",
    "        \n",
    "    counts = np.bincount(train_lgm)\n",
    "    class_weights = np.sqrt((1/(counts/np.max(counts))))\n",
    "    \n",
    "    cw_lgm = {}\n",
    "    \n",
    "    for i in range(len(class_weights)):\n",
    "        cw_lgm[i] = class_weights[i]\n",
    "    \n",
    "    # Get sample wise weights to enable multi-output-balancing\n",
    "    sw_id = np.array([cw_id[c] for c in train_y])\n",
    "    sw_lgm = np.array([cw_lgm[c] for c in train_lgm])\n",
    "\n",
    "\n",
    "    # Create and train the model\n",
    "    reset_seed()\n",
    "    sample_model = make_split_model(lr, out_dims = [len(np.unique(train_y)), len(np.unique(train_lgm))],\n",
    "                                losses = [\"sparse_categorical_crossentropy\", \"sparse_categorical_crossentropy\"])\n",
    "    sample_model.summary()\n",
    "\n",
    "    hist = sample_model.fit(train_x, {'ID': train_y, 'LGm': train_lgm}, sample_weight={'ID': sw_id, 'LGm': sw_lgm},\n",
    "                            epochs = epochs, batch_size = batch_size, validation_data = (val_x, [val_y, val_lgm]))\n",
    "\n",
    "    # Save history and model parameters\n",
    "    np.save(\"Results/BatchEffectReduction_\"+title+\"_hist_\"+str(en)+\".npy\", hist.history)\n",
    "    sample_model.save_weights(\"Models/BatchEffectReduction_\"+title+\"_Model_\"+str(en)+\".h5\")\n",
    "\n",
    "    return sample_model\n",
    "\n",
    "def sample_evaluation(sample_model, train_x, train_y, train_lgm, val_x, val_y, val_lgm, en, title, verbose = False):\n",
    "\n",
    "    # get performance on each sample\n",
    "    sample_dict = {}\n",
    "    \n",
    "    num_patients = len(np.unique(train_y))\n",
    "    for i in range(num_patients):\n",
    "    \n",
    "        # Get data from class i of the training set\n",
    "        d = np.expand_dims(np.squeeze(train_x[train_y == i]), axis = -1)\n",
    "        y = np.squeeze(train_y[train_y == i])\n",
    "        lgm = np.squeeze(train_lgm[train_y == i])\n",
    "        _, train_loss, _, train_acc, train_acc_lgm  = sample_model.evaluate(d, [y, lgm], batch_size = 256)\n",
    "        train_size = int(len(d))\n",
    "    \n",
    "        # Get data from class i of the validation set, these spectra have not been seen before\n",
    "        d = np.expand_dims(np.squeeze(val_x[val_y == i]), axis = -1)\n",
    "        y = np.squeeze(val_y[val_y == i])\n",
    "        lgm = np.squeeze(val_lgm[val_y == i])\n",
    "        _, val_loss, _, val_acc, val_acc_lgm  = sample_model.evaluate(d, [y, lgm], batch_size = 256)\n",
    "        val_size = int(len(d))\n",
    "\n",
    "    \n",
    "        sample_dict[i] = np.array([train_acc, train_acc_lgm, train_size, val_acc, val_acc_lgm, val_size])\n",
    "    \n",
    "    # Save the metrics in a dataframe\n",
    "    header = [\"Sample ID\", \"Train Accuracy\", \"Train LGm Accuracy\", \"Train Size\",\n",
    "              \"Validation Accuracy\", \"Validation LGm Accuracy\", \"Validation Size\"]\n",
    "    \n",
    "    df = pd.DataFrame.from_dict(sample_dict, columns = header[1:],\n",
    "                                            orient = \"index\")\n",
    "    # Round them to two decimals\n",
    "    df = df.round(decimals = 2)\n",
    "    \n",
    "    # Cast the number of spectra to integer, looks nicer than the decimal form with .0 after each number\n",
    "    df[\"Train Size\"] = df[\"Train Size\"].apply(lambda x: int(x))\n",
    "    df[\"Validation Size\"] = df[\"Validation Size\"].apply(lambda x: int(x))\n",
    "    \n",
    "    # Styling for convertion into latex format\n",
    "    df.style.set_table_styles([\n",
    "        {'selector': 'toprule', 'props': ':hline;'},\n",
    "        {'selector': 'midrule', 'props': ':hline;'},\n",
    "        {'selector': 'bottomrule', 'props': ':hline;'},\n",
    "    ], overwrite=False)\n",
    "\n",
    "    if verbose:\n",
    "        # Print the latex table, can be copied into the editor\n",
    "        latex = df.style.format(decimal=',', thousands='.', precision=2).to_latex(clines=\"all;data\",  column_format=\"|l|l|\")\n",
    "        print(latex)\n",
    "    \n",
    "    # Save metrics\n",
    "    np.save(\"Results/\"+title+\"_BatchEffectReduction_val_accuracy_\"+str(en)+\".npy\", df[\"Validation Accuracy\"].values)\n",
    "    np.save(\"Results/\"+title+\"_BatchEffectReduction_all_accuracies_\"+str(en)+\".npy\", df.values)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfad99a8-d063-42f3-ab45-aaa4d4b7423c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the folds\n",
    "organizer = StratifiedKFold(n_splits=n_folds, shuffle = True, random_state = 42)\n",
    "folds = organizer.split(X_all, y_all)\n",
    "gc.collect()\n",
    "\n",
    "plt.figure(figsize = (8, 8))\n",
    "for en, fold in enumerate(folds):\n",
    "    train_ix = fold[0]\n",
    "    val_ix = fold[1]\n",
    "\n",
    "    # Training set\n",
    "    train_x = X_all[train_ix]\n",
    "    train_y = y_all[train_ix]\n",
    "    train_lgm = lgm_all[train_ix]\n",
    "\n",
    "    # Validation set\n",
    "    val_x = X_all[val_ix]\n",
    "    val_y = y_all[val_ix]\n",
    "    val_lgm = lgm_all[val_ix]\n",
    "\n",
    "    # Data shuffle\n",
    "    ix = np.arange(len(train_x))\n",
    "    np.random.shuffle(ix)\n",
    "    train_x = train_x[ix]\n",
    "    train_y = train_y[ix]\n",
    "    train_lgm = train_lgm[ix]\n",
    "\n",
    "    enc = make_encoder()\n",
    "    enc.summary()\n",
    "    path = \"Models/data_encoders/\"+title+\"_MutantVsWildtype_importance\"+str(en)+\".h5\"\n",
    "    enc.load_weights(path, by_name = False)\n",
    "        \n",
    "    X = enc.predict(train_x)\n",
    "    X_v = enc.predict(val_x)\n",
    "    sample_model = TrainModel(X, train_y, train_lgm, X_v, val_y, val_lgm, en, title = title)\n",
    "\n",
    "\n",
    "    # get performance on each sample\n",
    "    sample_evaluation(sample_model, X, train_y, train_lgm, X_v, val_y, val_lgm, en, title = title, verbose = False)\n",
    "\n",
    "\n",
    "    del train_x\n",
    "    del train_y\n",
    "    del val_x\n",
    "    del val_y\n",
    "    X, X_v\n",
    "    del sample_model\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9cf49ac-c6e7-4030-8459-d23909bd022a",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_v.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4be3a40-ad0a-4c37-9edf-22c1c79b3416",
   "metadata": {},
   "outputs": [],
   "source": [
    "id_dict = {}\n",
    "lgm_dict = {}\n",
    "\n",
    "for title in [\"MANUAL\", \"RAW\"]:\n",
    "\n",
    "    accs = []\n",
    "    for en in range(n_folds):\n",
    "        #hist = np.load(\"Results/FullTest_\"+title+\"_hist_\"+str(en)+\".npy\", allow_pickle = True)\n",
    "        acc = np.load(\"Results/\"+title+\"all_accuracies_\"+str(en)+\".npy\", allow_pickle = True)\n",
    "        accs.append(acc[:, -3:-1])\n",
    "\n",
    "    accs = np.array(accs)\n",
    "\n",
    "    mean = np.mean(accs, axis = 0)\n",
    "    id_dict[title] = mean.T[0]\n",
    "    lgm_dict[title] = mean.T[1]\n",
    "\n",
    "accs = []\n",
    "title = \"MANUAL\"\n",
    "for en in range(n_folds):\n",
    "    acc = np.load(\"Results/\"+title+\"_BatchEffectReduction_all_accuracies_\"+str(en)+\".npy\")\n",
    "    accs.append(acc[:, -3:-1])\n",
    "\n",
    "accs = np.array(accs)\n",
    "\n",
    "mean = np.mean(accs, axis = 0)\n",
    "id_dict[title+\"_After\"] = mean.T[0]\n",
    "lgm_dict[title+\"_After\"] = mean.T[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5729bf35-2b8a-4e87-942a-560e3939372e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.stats as stats\n",
    "\n",
    "plt.rcParams.update({'font.size': 40})\n",
    "plt.rcParams[\"font.family\"] = \"Times New Roman\"\n",
    "\n",
    "################\n",
    "# ID accuracies\n",
    "################\n",
    "\n",
    "sorting = np.argsort(id_dict[\"MANUAL_After\"])\n",
    "x_range = np.arange(len(id_dict[\"RAW\"]))\n",
    "\n",
    "plt.figure(figsize = (10, 5))\n",
    "\n",
    "\n",
    "conf_level = 0.95\n",
    "d = id_dict[\"RAW\"]\n",
    "RAW_cl = stats.t.interval(conf_level, df=len(d)-1, loc=np.mean(d), scale=np.std(d, ddof=1) / np.sqrt(len(d)))\n",
    "RAW_delta = np.round(np.max(np.abs(np.mean(d) - RAW_cl)), 3)\n",
    "d = id_dict[\"MANUAL\"]\n",
    "MANUAL_cl = stats.t.interval(conf_level, df=len(d)-1, loc=np.mean(d), scale=np.std(d, ddof=1) / np.sqrt(len(d)))\n",
    "MANUAL_delta = np.round(np.max(np.abs(np.mean(d) - MANUAL_cl)), 3)\n",
    "d = id_dict[\"MANUAL_After\"]\n",
    "RADAR_cl = stats.t.interval(conf_level, df=len(d)-1, loc=np.mean(d), scale=np.std(d, ddof=1) / np.sqrt(len(d)))\n",
    "RADAR_delta = np.round(np.max(np.abs(np.mean(d) - RADAR_cl)), 3)\n",
    "\n",
    "\n",
    "plt.scatter(x_range, id_dict[\"RAW\"][sorting], label = \"Raw data:\" + str(np.round(np.mean(id_dict[\"RAW\"]), 2)) + \", CI: ± \" + str(RAW_delta))\n",
    "plt.scatter(x_range, id_dict[\"MANUAL\"][sorting], label = \"Manual Before:\" + str(np.round(np.mean(id_dict[\"MANUAL\"]), 2)) + \", CI: ± \" + str(MANUAL_delta))\n",
    "plt.scatter(x_range, id_dict[\"MANUAL_After\"][sorting], label = \"Manual After:\" + str(np.round(np.mean(id_dict[\"MANUAL_After\"]), 2)) + \", CI: ± \" + str(RADAR_delta))\n",
    "\n",
    "plt.ylim([-0.1, 1.1])\n",
    "plt.legend(fontsize = 20, loc = \"upper left\")\n",
    "\n",
    "plt.savefig(\"Images/Histories/Corrected_ID_ACC_comparison.png\", format=\"png\", transparent = True,\n",
    "                    dpi = 1000,\n",
    "                    bbox_inches='tight',\n",
    "                    pad_inches=0.5)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "################\n",
    "# LGm accuracies\n",
    "################\n",
    "\n",
    "sorting = np.argsort(lgm_dict[\"MANUAL_After\"])\n",
    "\n",
    "\n",
    "d = lgm_dict[\"RAW\"]\n",
    "RAW_cl = stats.t.interval(conf_level, df=len(d)-1, loc=np.mean(d), scale=np.std(d, ddof=1) / np.sqrt(len(d)))\n",
    "RAW_delta = np.round(np.max(np.abs(np.mean(d) - RAW_cl)), 3)\n",
    "d = lgm_dict[\"MANUAL\"]\n",
    "MANUAL_cl = stats.t.interval(conf_level, df=len(d)-1, loc=np.mean(d), scale=np.std(d, ddof=1) / np.sqrt(len(d)))\n",
    "MANUAL_delta = np.round(np.max(np.abs(np.mean(d) - MANUAL_cl)), 3)\n",
    "d = lgm_dict[\"MANUAL_After\"]\n",
    "RADAR_cl = stats.t.interval(conf_level, df=len(d)-1, loc=np.mean(d), scale=np.std(d, ddof=1) / np.sqrt(len(d)))\n",
    "RADAR_delta = np.round(np.max(np.abs(np.mean(d) - RADAR_cl)), 3)\n",
    "\n",
    "\n",
    "plt.figure(figsize = (10, 5))\n",
    "\n",
    "plt.scatter(x_range, lgm_dict[\"RAW\"][sorting], label = \"Raw data:\" + str(np.round(np.mean(lgm_dict[\"RAW\"]), 2)) + \", CI: ± \" + str(RAW_delta))\n",
    "plt.scatter(x_range, lgm_dict[\"MANUAL\"][sorting], label = \"Manual Before:\" + str(np.round(np.mean(lgm_dict[\"MANUAL\"]), 2)) + \", CI: ± \" + str(MANUAL_delta))\n",
    "plt.scatter(x_range, lgm_dict[\"MANUAL_After\"][sorting], label = \"Manual After:\" + str(np.round(np.mean(lgm_dict[\"MANUAL_After\"]), 2)) + \", CI: ± \" + str(RADAR_delta))\n",
    "\n",
    "plt.ylim([-0.1, 1.1])\n",
    "plt.legend(fontsize = 20, loc = \"lower right\")\n",
    "\n",
    "plt.savefig(\"Images/Histories/Corrected_LGM_ACC_comparison.png\", format=\"png\", transparent = True,\n",
    "                    dpi = 1000,\n",
    "                    bbox_inches='tight',\n",
    "                    pad_inches=0.5)\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21e0d274-86a8-4426-8007-602c94fa77cd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
