{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dc49445-c853-4231-87a7-6323753f9696",
   "metadata": {},
   "outputs": [],
   "source": [
    "from Scripts.essentials import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a86e5db3-3551-40a9-a506-ab14c608cab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "p = \"Data/\"\n",
    "train_x = np.load(p + \"train_x_MANUAL.npy\")\n",
    "val_x = np.load(p + \"val_x_MANUAL.npy\")\n",
    "\n",
    "train_y = np.load(p + \"train_y_46.npy\")\n",
    "val_y = np.load(p + \"val_y_46.npy\")\n",
    "\n",
    "train_lgm = np.load(p + \"train_lgm.npy\")\n",
    "val_lgm = np.load(p + \"val_lgm.npy\")\n",
    "\n",
    "np.random.seed(0)\n",
    "ix = np.arange(len(train_x))\n",
    "np.random.shuffle(ix)\n",
    "train_x = train_x[ix]\n",
    "train_y = train_y[ix]\n",
    "train_lgm = train_lgm[ix]\n",
    "\n",
    "ix = np.arange(len(val_x))\n",
    "np.random.shuffle(ix)\n",
    "val_x = val_x[ix]\n",
    "val_y = val_y[ix]\n",
    "val_lgm = val_lgm[ix]\n",
    "\n",
    "del ix\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ce73298-b45d-4ed3-bcfb-436d45a75dd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_lgm = np.argmax(train_lgm, axis = 1)\n",
    "val_lgm = np.argmax(val_lgm, axis = 1)\n",
    "train_lgm = np.where(train_lgm > 2, 0, 1)\n",
    "val_lgm = np.where(val_lgm > 2, 0, 1)\n",
    "\n",
    "eye = np.eye(2)\n",
    "\n",
    "train_lgm = eye[train_lgm]\n",
    "val_lgm = eye[val_lgm]\n",
    "\n",
    "print(train_lgm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6701c709-2ab0-4b6b-9629-a4a7f267fa11",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_subset = np.arange(len(np.unique(np.argmax(train_y, axis = 1))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7537889a-6373-4d6b-8c55-df3d8f0e7d92",
   "metadata": {},
   "outputs": [],
   "source": [
    "counts = np.bincount(np.argmax(train_y, axis = 1))\n",
    "class_weights = np.sqrt((1/(counts/np.max(counts))))\n",
    "\n",
    "cw = {}\n",
    "\n",
    "for i in range(len(class_weights)):\n",
    "    cw[i] = class_weights[i]\n",
    "\n",
    "print(cw)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec6cce29-b8ba-4a1c-b041-715da6a631d9",
   "metadata": {},
   "source": [
    "# Train a split model to deduce features which ignores patient id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e20d770-052a-4b48-9d97-a9dbc28175d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "enc, bias_model = make_encoder(), make_split_model(out_dims = [len(train_y[0]), len(train_lgm[0])])\n",
    "enc.summary()\n",
    "bias_model.summary()\n",
    "combined_model = make_combined_model(enc, bias_model)\n",
    "combined_model.summary()\n",
    "\n",
    "del enc, bias_model, combined_model\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "976dcaf5-4f69-492b-896e-5ebadfaa464c",
   "metadata": {},
   "outputs": [],
   "source": [
    "repeats = 300\n",
    "epochs = 1\n",
    "batch_size = 256\n",
    "lr = 0.00005\n",
    "decay_rate = 0.003\n",
    "lr_scaler = 0.05\n",
    "\n",
    "hist_I = []\n",
    "hist_II = []\n",
    "reset_seed()\n",
    "enc, bias_model = make_encoder(), make_split_model(out_dims = [len(sample_subset), 2])\n",
    "\n",
    "importances = []\n",
    "imp = enc.get_layer(\"importance\").importance.numpy()\n",
    "importances.append(imp)\n",
    "\n",
    "transformations = []\n",
    "\n",
    "transf = np.squeeze(enc.predict(np.expand_dims(np.mean(train_x, axis = 0), 0)))\n",
    "transformations.append(transf)\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize = (15, 7))\n",
    "ax[0].plot(imp)\n",
    "ax[0].set_ylim([0, 1])\n",
    "    \n",
    "ax[1].plot(transf)\n",
    "\n",
    "\n",
    "del transf\n",
    "gc.collect()\n",
    "\n",
    "all_transformations = []\n",
    "\n",
    "transf = np.squeeze(enc.predict(np.expand_dims(train_x[:1024], 0)))\n",
    "std = np.std(transf, axis = 0)\n",
    "mean = np.mean(transf, axis = 0)\n",
    "all_transformations.append([mean - std, mean + std])\n",
    "\n",
    "ax[1].fill_between(np.arange(1738), mean - std, mean + std, alpha = 0.5, color = \"black\")\n",
    "ax[1].set_ylim([0, 1])\n",
    " \n",
    "plt.show()\n",
    "\n",
    "del transf, mean, std, imp\n",
    "\n",
    "gc.collect()\n",
    "histories = []\n",
    "\n",
    "\n",
    "for repeat in range(repeats):\n",
    "    \n",
    "    print(\"Repeat:\", str(repeat+1), \", alpha:\", str(lr))\n",
    "\n",
    "    # I\n",
    "    enc.trainable = False\n",
    "    bias_model.trainable = True\n",
    "    reset_seed()\n",
    "    split_model = make_combined_model(enc, bias_model,\n",
    "                                         lr = lr,\n",
    "                                         losses = [\"categorical_crossentropy\", \"categorical_crossentropy\"])\n",
    "    \n",
    "    print(\"Train the id and MutWt models\")\n",
    "    hist_I.append(split_model.fit(train_x,\n",
    "                    [train_y, train_lgm],\n",
    "                    batch_size = batch_size,\n",
    "                    epochs = epochs,\n",
    "                   validation_data=(val_x, [val_y, val_lgm])\n",
    "                   ).history\n",
    "    )\n",
    "    \n",
    "    # II\n",
    "    enc.trainable = True\n",
    "    bias_model.trainable = False\n",
    "    reset_seed()\n",
    "    split_model = make_combined_model(enc, bias_model,\n",
    "                                         lr = lr,\n",
    "                                         losses = [negative_CE, \"categorical_crossentropy\"],\n",
    "                                         metrics = [\"accuracy\"])\n",
    "    \n",
    "    print(\"Train the encoder to decrease id accuracy and maintain MutWt accuracy\")\n",
    "    hist_II.append(split_model.fit(train_x, \n",
    "                [train_y, train_lgm],\n",
    "                batch_size = batch_size,\n",
    "                epochs = epochs,\n",
    "                validation_data=(val_x, [val_y, val_lgm]) # Provide true patient ids to the validation to see how accuracy decreases on unseen data\n",
    "                ).history\n",
    "                  )\n",
    "\n",
    "    # Gather metrics and signals for plotting the gif later\n",
    "    imp = enc.get_layer(\"importance\").importance.numpy()\n",
    "    importances.append(imp)\n",
    "\n",
    "    transf = np.squeeze(enc.predict(np.expand_dims(np.mean(train_x, axis = 0), 0)))\n",
    "    transformations.append(transf)\n",
    "\n",
    "    fig, ax = plt.subplots(1, 2, figsize = (15, 7))\n",
    "    ax[0].plot(imp)\n",
    "    ax[0].set_ylim([0, 1])\n",
    "    \n",
    "    \n",
    "    \n",
    "    p = split_model.predict(val_x)\n",
    "    y_1 = np.argmax(p[0], axis = 1)\n",
    "    y_2 = np.argmax(p[1], axis = 1)\n",
    "    \n",
    "    h1 = balanced_accuracy_score(np.argmax(val_y, axis = 1), y_1)\n",
    "    h2 = balanced_accuracy_score(np.argmax(val_lgm, axis = 1), y_2)\n",
    "    \n",
    "    histories.append([h1, h2])\n",
    "    ax[1].plot(transf)\n",
    "\n",
    "    del transf\n",
    "\n",
    "    transf = np.squeeze(enc.predict(np.expand_dims(train_x[:1024], 0)))\n",
    "    std = np.std(transf, axis = 0)\n",
    "    mean = np.mean(transf, axis = 0)\n",
    "    all_transformations.append([mean - std, mean + std])\n",
    "\n",
    "    ax[1].fill_between(np.arange(1738), mean - std, mean + std, alpha = 0.5, color = \"black\")\n",
    "    ax[1].set_ylim([0, 1])\n",
    "    \n",
    "    \n",
    "    plt.show()\n",
    "    \n",
    "    del split_model\n",
    "    del transf, mean, std, imp\n",
    "    gc.collect()\n",
    "\n",
    "    lr = lr - (lr * decay_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f5a38d9-54b6-451a-a016-5ab9de08619f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(enc.weights)):\n",
    "    enc.weights[i]._handle_name = enc.weights[i].name + \"_\" + str(i)\n",
    "np.save(\"Results/Features/(MANUAL)MutantVsWildtype_importance.npy\", enc.get_layer(\"importance\").importance.numpy())\n",
    "enc.save_weights(\"Models/data_encoders/(MANUAL)MutantVsWildtype_importance.h5\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "582f73ba-0763-4d30-b635-04bdc20189ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "enc.layers[-1].maximum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c349e57-4fc0-4866-bde0-f3f0ffb87347",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.animation import FuncAnimation\n",
    "import io\n",
    "from PIL import Image\n",
    "plt.rcParams.update({'font.size': 20})\n",
    "spec = train_x[0]\n",
    "\n",
    "# Create animation of feature importance vector evolution\n",
    "def plotImp(i):\n",
    "    plt.clf()\n",
    "    fig, ax = plt.subplots(2, figsize = (20, 10))\n",
    "\n",
    "\n",
    "    mean = np.mean(train_x, axis = 0) * importances[i]\n",
    "\n",
    "    #ax[0].plot(mean, color = \"Red\", alpha = 0.5)\n",
    "    ax[0].plot(importances[i], alpha =  0.1, linestyle = \"--\", color = \"Blue\")\n",
    "             \n",
    "    ax[0].scatter(np.arange(1738), importances[i], color = \"blue\", alpha = 0.2, s = 5) # Scatter all features\n",
    "    ax[0].set_ylim([0, 1.05])\n",
    "    \n",
    "    ax[1].fill_between(np.arange(len(transformations[i])), all_transformations[i][0], all_transformations[i][1])\n",
    "    ax[1].plot(transformations[i], color = \"red\", alpha = 0.5)\n",
    "    ax[1].set_ylim([0, 1.05])\n",
    "    \n",
    "    plt.title(\"Epoch: \" + str(i) + \"    Val ID Acc: \" + str(np.round(histories[i][0], 2)) + \"    Val LGm Acc: \" + str(np.round(histories[i][1], 2)))\n",
    "            \n",
    "\n",
    "fig = plt.figure(figsize=(14, 7))\n",
    "frames = []\n",
    "for i in range(len(importances)-1):\n",
    "    plotImp(i)\n",
    "    buf = io.BytesIO()\n",
    "    plt.savefig(buf, format=\"png\")\n",
    "    plt.close()\n",
    "    buf.seek(0)\n",
    "    frames.append(Image.open(buf))\n",
    "from IPython.display import Image\n",
    "\n",
    "# Create and save the animated GIF\n",
    "frames[0].save(\n",
    "    \"(MANUAL)importancesAPOLLOunimportantFeatures.gif\",\n",
    "    save_all=True,\n",
    "    append_images=frames[1:],\n",
    "    duration=100,\n",
    "    loop=0,\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15cca6d6-526b-4e18-a916-8d89d5ea28a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "Image(\"(MANUAL)importancesAPOLLOunimportantFeatures.gif\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be0df87c-a71f-4a22-9889-a43f9edffc00",
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0c9d7fc-02ab-4a46-8555-694f07f23882",
   "metadata": {},
   "outputs": [],
   "source": [
    "t = enc.predict(val_x)\n",
    "t_mt = t[np.argmax(val_lgm, axis = 1) == 0]\n",
    "\n",
    "plt.fill_between(np.arange(1738), np.min(t_mt, axis = 0), np.max(t_mt, axis = 0), color = \"red\", alpha = 0.5)\n",
    "\n",
    "\n",
    "t_wt = t[np.argmax(val_lgm, axis = 1) == 1]\n",
    "\n",
    "plt.fill_between(np.arange(1738), np.min(t_wt, axis = 0), np.max(t_wt, axis = 0), color = \"blue\", alpha = 0.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b19c85f-4f5f-4542-ac21-d3cc4e71bdec",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams.update({'font.size': 40})\n",
    "plt.rcParams[\"font.family\"] = \"Times New Roman\"\n",
    "\n",
    "imp = enc.get_layer(\"importance\").importance.numpy()\n",
    "np.save(\"Results/BiasFeatures/(MANUAL)FeatureImportances(biasPresence(corrected_data)).npy\", imp)\n",
    "clean_imp = imp/np.max(imp)\n",
    "threshold = 0.001\n",
    "most_important = np.where(clean_imp > threshold)[0]\n",
    "\n",
    "plt.figure(figsize= (15, 7))\n",
    "\n",
    "\n",
    "\n",
    "plt.plot(np.mean(train_x, axis = 0), color = \"Black\", label = \"Mean spectrum (training data)\")\n",
    "\n",
    "\n",
    "# Dots on the medium spectrum indicating position of important feature\n",
    "plt.scatter(most_important, [np.mean(train_x, axis = 0)[most_important]], color = \"Red\", alpha = 0.5,)\n",
    "\n",
    "plt.legend(fontsize = 30)\n",
    "plt.savefig(\"Images/Features/(MANUAL)BiasFeatureImportanceIndices.png\", format=\"png\", transparent = True,\n",
    "                    dpi = 1000,\n",
    "                    bbox_inches='tight',\n",
    "                    pad_inches=0.5)\n",
    "\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize= (15, 7))\n",
    "\n",
    "# Show the entire importance vector\n",
    "plt.plot(clean_imp, color = \"Red\", alpha = 0.5)\n",
    "\n",
    "\n",
    "plt.yticks([0, 0.5, 1])\n",
    "plt.savefig(\"Images/Features/(MANUAL)BiasFeatureImportance.png\", format=\"png\", transparent = True,\n",
    "                    dpi = 1000,\n",
    "                    bbox_inches='tight',\n",
    "                    pad_inches=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ba4e75c-0759-46dc-aa56-5281fb655aee",
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
