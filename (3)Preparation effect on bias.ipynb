{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dc49445-c853-4231-87a7-6323753f9696",
   "metadata": {},
   "outputs": [],
   "source": [
    "from Scripts.essentials import *\n",
    "\n",
    "epochs = 300\n",
    "batch_size = 256\n",
    "lr = 0.00005\n",
    "\n",
    "early_stop = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor=\"val_loss\",\n",
    "    min_delta=0,\n",
    "    patience = 4,\n",
    "    verbose=0,\n",
    "    mode=\"auto\",\n",
    "    baseline=None,\n",
    "    restore_best_weights=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01413051-1618-4ab4-a299-c82dc0c29638",
   "metadata": {},
   "source": [
    "# Simple example of how applying preprocessing to data can help with the spectrum effect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9666b070-8219-47ff-9194-0572b4d6dc2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "plt.rcParams.update({'font.size': 40})\n",
    "plt.rcParams[\"font.family\"] = \"Times New Roman\"\n",
    "#fig, ax = plt.subplots(4, 2, figsize = (20, 20))\n",
    "p = \"Data/\"\n",
    "train_x = np.load(p + \"train_x.npy\")\n",
    "val_x = np.load(p + \"val_x.npy\")\n",
    "\n",
    "train_y = np.load(p + \"train_y.npy\")\n",
    "val_y = np.load(p + \"val_y.npy\")\n",
    "\n",
    "HF_1887 = train_x[np.argmax(train_y, axis = 1) == 10] # Get sample spectra\n",
    "d = HF_1887\n",
    "min_ = np.min(d, axis = 0)\n",
    "max_ = np.max(d, axis = 0)\n",
    "sd = np.std(d, axis = 0)\n",
    "mean = np.mean(d, axis = 0)\n",
    "\n",
    "plt.fill_between(np.arange(1738), mean - sd, mean + sd, alpha = 1, color = \"Red\")\n",
    "plt.plot(mean, linestyle = \"--\", color = \"black\")\n",
    "plt.savefig(\"Images/(MANUAL)SolveSpectrumEffect_1887.png\", format=\"png\", transparent = True,\n",
    "                    dpi = 300,\n",
    "                    bbox_inches='tight',\n",
    "                    pad_inches=0.5)\n",
    "plt.show()\n",
    "\n",
    "# Get the other sample\n",
    "HF_2070 = train_x[np.argmax(train_y, axis = 1) == 17]\n",
    "d = HF_2070\n",
    "min_ = np.min(d, axis = 0)\n",
    "max_ = np.max(d, axis = 0)\n",
    "sd = np.std(d, axis = 0)\n",
    "mean = np.mean(d, axis = 0)\n",
    "\n",
    "plt.fill_between(np.arange(1738), mean - sd, mean + sd, alpha = 1, color = \"Red\")\n",
    "plt.plot(mean, linestyle = \"--\", color = \"black\")\n",
    "plt.savefig(\"Images/(MANUAL)SolveSpectrumEffect_2070.png\", format=\"png\", transparent = True,\n",
    "                    dpi = 300,\n",
    "                    bbox_inches='tight',\n",
    "                    pad_inches=0.5)\n",
    "plt.show()\n",
    "\n",
    "# Make a model and show how easy it is to learn the difference\n",
    "reset_seed()\n",
    "model = make_model(lr = lr, out_dim = 2, reg_param = 1e-7)\n",
    "model.summary()\n",
    "\n",
    "\n",
    "HF_1887_val = val_x[np.argmax(val_y, axis = 1) == 10]\n",
    "\n",
    "HF_2070_val = val_x[np.argmax(val_y, axis = 1) == 17]\n",
    "\n",
    "\n",
    "d = np.concatenate([HF_1887, HF_2070])\n",
    "l = np.concatenate([np.zeros(len(HF_1887)), np.ones(len(HF_2070))])\n",
    "\n",
    "d_val = np.concatenate([HF_1887_val, HF_2070_val])\n",
    "l_val = np.concatenate([np.zeros(len(HF_1887_val)), np.ones(len(HF_2070_val))])\n",
    "\n",
    "ix = np.arange(len(d))\n",
    "np.random.shuffle(ix)\n",
    "\n",
    "d = d[ix]\n",
    "l = l[ix]\n",
    "\n",
    "print(d.shape)\n",
    "print(\"Sample ratio:\", str(len(HF_1887)/(len(HF_2070))))\n",
    "\n",
    "hist1 = model.fit(d, l, epochs = epochs, batch_size = batch_size, validation_data = [d_val, l_val])\n",
    "\n",
    "plt.rcParams.update({'font.size': 30})\n",
    "# Evaluate the model on validation data\n",
    "preds = np.argmax(model.predict(d_val), axis = 1)\n",
    "print(\"Balanced accuracy:\", balanced_accuracy_score(l_val, preds))\n",
    "fpr, tpr, thresholds = metrics.roc_curve(l_val, preds)\n",
    "roc_auc = metrics.auc(fpr, tpr)\n",
    "display = metrics.RocCurveDisplay(fpr=fpr, tpr=tpr, roc_auc=roc_auc, estimator_name='')\n",
    "display.plot()\n",
    "plt.savefig(\"Images/(MANUAL)SolveSpectrumEffect_unprep_auroc.png\", format=\"png\", transparent = True,\n",
    "                    dpi = 300,\n",
    "                    bbox_inches='tight',\n",
    "                    pad_inches=0.5)\n",
    "plt.show()\n",
    "\n",
    "plt.rcParams.update({'font.size': 40})\n",
    "# Plot training process\n",
    "plt.plot(hist1.history[\"loss\"], label = \"Training\")\n",
    "plt.plot(hist1.history[\"val_loss\"], label = \"Validation\")\n",
    "plt.ylim([-0.05, 1.25])\n",
    "plt.savefig(\"Images/(MANUAL)SolveSpectrumEffect_unprep_loss.png\", format=\"png\", transparent = True,\n",
    "                    dpi = 300,\n",
    "                    bbox_inches='tight',\n",
    "                    pad_inches=0.5)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Load the manually preprocessed data\n",
    "train_x = np.load(p + \"train_x_MANUAL.npy\")\n",
    "val_x = np.load(p + \"val_x_MANUAL.npy\")\n",
    "\n",
    "\n",
    "HF_1887 = train_x[np.argmax(train_y, axis = 1) == 10]\n",
    "d = HF_1887\n",
    "min_ = np.min(d, axis = 0)\n",
    "max_ = np.max(d, axis = 0)\n",
    "sd = np.std(d, axis = 0)\n",
    "mean = np.mean(d, axis = 0)\n",
    "\n",
    "plt.fill_between(np.arange(1738), mean - sd, mean + sd, alpha = 1, color = \"Red\")\n",
    "plt.plot(mean, linestyle = \"--\", color = \"black\")\n",
    "\n",
    "plt.savefig(\"Images/(MANUAL)SolveSpectrumEffect_1887(prep).png\", format=\"png\", transparent = True,\n",
    "                    dpi = 300,\n",
    "                    bbox_inches='tight',\n",
    "                    pad_inches=0.5)\n",
    "plt.show()\n",
    "\n",
    "HF_2070 = train_x[np.argmax(train_y, axis = 1) == 17]\n",
    "d = HF_2070\n",
    "min_ = np.min(d, axis = 0)\n",
    "max_ = np.max(d, axis = 0)\n",
    "sd = np.std(d, axis = 0)\n",
    "mean = np.mean(d, axis = 0)\n",
    "\n",
    "plt.fill_between(np.arange(1738), mean - sd, mean + sd, alpha = 1, color = \"Red\")\n",
    "plt.plot(mean, linestyle = \"--\", color = \"black\")\n",
    "\n",
    "plt.savefig(\"Images/(MANUAL)SolveSpectrumEffect_2070(prep).png\", format=\"png\", transparent = True,\n",
    "                    dpi = 300,\n",
    "                    bbox_inches='tight',\n",
    "                    pad_inches=0.5)\n",
    "plt.show()\n",
    "reset_seed()\n",
    "model = make_model(lr = lr, out_dim = 2, reg_param = 1e-7)\n",
    "\n",
    "HF_1887_val = val_x[np.argmax(val_y, axis = 1) == 10]\n",
    "\n",
    "HF_2070_val = val_x[np.argmax(val_y, axis = 1) == 17]\n",
    "\n",
    "\n",
    "d = np.concatenate([HF_1887, HF_2070])\n",
    "l = np.concatenate([np.zeros(len(HF_1887)), np.ones(len(HF_2070))])\n",
    "\n",
    "d_val = np.concatenate([HF_1887_val, HF_2070_val])\n",
    "l_val = np.concatenate([np.zeros(len(HF_1887_val)), np.ones(len(HF_2070_val))])\n",
    "\n",
    "ix = np.arange(len(d))\n",
    "np.random.shuffle(ix)\n",
    "\n",
    "d = d[ix]\n",
    "l = l[ix]\n",
    "\n",
    "hist2 = model.fit(d, l, epochs = epochs, batch_size = batch_size, validation_data = [d_val, l_val])\n",
    "\n",
    "# Evaluate the model on validation data\n",
    "preds = np.argmax(model.predict(d_val), axis = 1)\n",
    "print(\"Balanced accuracy:\", balanced_accuracy_score(l_val, preds))\n",
    "fpr, tpr, thresholds = metrics.roc_curve(l_val, preds)\n",
    "\n",
    "roc_auc = metrics.auc(fpr, tpr)\n",
    "plt.rcParams.update({'font.size': 30})\n",
    "display = metrics.RocCurveDisplay(fpr=fpr, tpr=tpr, roc_auc=roc_auc, estimator_name='')\n",
    "display.plot()\n",
    "\n",
    "plt.savefig(\"Images/(MANUAL)SolveSpectrumEffect_prep_auroc.png\", format=\"png\", transparent = True,\n",
    "                    dpi = 300,\n",
    "                    bbox_inches='tight',\n",
    "                    pad_inches=0.5)\n",
    "plt.show()\n",
    "\n",
    "plt.rcParams.update({'font.size': 40})\n",
    "plt.plot(hist2.history[\"loss\"], label = \"Training\")\n",
    "plt.plot(hist2.history[\"val_loss\"], label = \"Validation\")\n",
    "plt.ylim([-0.05, 1.25])\n",
    "plt.savefig(\"Images/(MANUAL)SolveSpectrumEffect_prep_loss.png\", format=\"png\", transparent = True,\n",
    "                    dpi = 300,\n",
    "                    bbox_inches='tight',\n",
    "                    pad_inches=0.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91d1d730-0bc5-4425-8695-8735cd07b64d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the labels with tumors joined according to the mother tumor\n",
    "train_y = np.load(p + \"train_y_46.npy\")\n",
    "\n",
    "\n",
    "# Compute the weights of each unique label relative to their frequencies\n",
    "counts = np.bincount(np.argmax(train_y, axis = 1))\n",
    "class_weights = np.sqrt((1/(counts/np.max(counts))))\n",
    "\n",
    "cw = {}\n",
    "\n",
    "for i in range(len(class_weights)):\n",
    "    cw[i] = class_weights[i]\n",
    "    print(i,\":\", cw[i], \"(\", counts[i], \" spectra in training set)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e364c492-62cf-42ab-9a26-499363851802",
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect()\n",
    "p = \"Data/\"\n",
    "train_x = np.load(p + \"train_x.npy\")\n",
    "test_x = np.load(p + \"test_x.npy\")\n",
    "val_x = np.load(p + \"val_x.npy\")\n",
    "\n",
    "train_y = np.load(p + \"train_y_46.npy\")\n",
    "test_y = np.load(p + \"test_y_46.npy\")\n",
    "val_y = np.load(p + \"val_y_46.npy\")\n",
    "\n",
    "train_lgm = np.load(p + \"train_lgm.npy\")\n",
    "test_lgm = np.load(p + \"test_lgm.npy\")\n",
    "val_lgm = np.load(p + \"val_lgm.npy\")\n",
    "\n",
    "# Binary encoding from lgm to mutant vs. wildtype\n",
    "train_lgm = np.argmax(train_lgm, axis = 1)\n",
    "test_lgm = np.argmax(test_lgm, axis = 1)\n",
    "val_lgm = np.argmax(val_lgm, axis = 1)\n",
    "\n",
    "train_lgm = np.where(train_lgm > 2, 0, 1)\n",
    "test_lgm = np.where(test_lgm > 2, 0, 1)\n",
    "val_lgm = np.where(val_lgm > 2, 0, 1)\n",
    "\n",
    "eye = np.eye(2)\n",
    "\n",
    "train_lgm = eye[train_lgm]\n",
    "val_lgm = eye[val_lgm]\n",
    "test_lgm = eye[test_lgm]\n",
    "\n",
    "\n",
    "np.random.seed(0)\n",
    "ix = np.arange(len(train_x))\n",
    "np.random.shuffle(ix)\n",
    "train_x = train_x[ix]\n",
    "train_y = train_y[ix]\n",
    "train_lgm = train_lgm[ix]\n",
    "\n",
    "# Train and save the model\n",
    "reset_seed()\n",
    "sample_model = make_split_model(lr, out_dims = [len(train_y[0]), len(train_lgm[0])])\n",
    "sample_model.summary()\n",
    "hist = sample_model.fit(train_x, [train_y, train_lgm], epochs = epochs,\n",
    "                        batch_size = batch_size, validation_data = (val_x, [val_y, val_lgm]), callbacks = [early_stop])\n",
    "sample_model.save_weights(\"Models\\Raw_Bias_quantifier.h5\")\n",
    "\n",
    "\n",
    "# or save to csv: \n",
    "hist_csv_file = 'Raw_bias_history.csv'\n",
    "with open(hist_csv_file, mode='w') as f:\n",
    "    hist_df.to_csv(f)\n",
    "\n",
    "# Show metrics on the datasets\n",
    "print(\"ID balanced accuracy\")\n",
    "preds = np.argmax(sample_model.predict(train_x)[0], axis = 1)\n",
    "print(\"Train:\", np.round(balanced_accuracy_score(np.argmax(train_y, axis = 1), preds), 2))\n",
    "\n",
    "preds = np.argmax(sample_model.predict(val_x)[0], axis = 1)\n",
    "print(\"Validation:\", np.round(balanced_accuracy_score(np.argmax(val_y, axis = 1), preds), 2))\n",
    "\n",
    "preds = np.argmax(sample_model.predict(test_x)[0], axis = 1)\n",
    "print(\"Test:\", np.round(balanced_accuracy_score(np.argmax(test_y, axis = 1), preds), 2))\n",
    "\n",
    "print(\"LGM balanced accuracy\")\n",
    "preds = np.argmax(sample_model.predict(train_x)[1], axis = 1)\n",
    "print(\"Train:\", np.round(balanced_accuracy_score(np.argmax(train_lgm, axis = 1), preds), 2))\n",
    "\n",
    "preds = np.argmax(sample_model.predict(val_x)[1], axis = 1)\n",
    "print(\"Validation:\", np.round(balanced_accuracy_score(np.argmax(val_lgm, axis = 1), preds), 2))\n",
    "\n",
    "preds = np.argmax(sample_model.predict(test_x)[1], axis = 1)\n",
    "print(\"Test:\", np.round(balanced_accuracy_score(np.argmax(test_lgm, axis = 1), preds), 2))\n",
    "# Plot training process\n",
    "\n",
    "# Loss\n",
    "plt.rcParams.update({'font.size': 40})\n",
    "plt.rcParams[\"font.family\"] = \"Times New Roman\"\n",
    "plt.plot(hist.history[\"loss\"], label = \"Training\")\n",
    "plt.plot(hist.history[\"val_loss\"], label = \"Validation\")\n",
    "plt.title(\"Loss\")\n",
    "plt.legend(fontsize = 30)\n",
    "plt.savefig(\"Images/Histories/NOPREPBiasLoss.png\", format=\"png\", transparent = True,\n",
    "                    dpi = 1000,\n",
    "                    bbox_inches='tight',\n",
    "                    pad_inches=0.5)\n",
    "plt.show()\n",
    "\n",
    "# Accuracies\n",
    "plt.plot(hist.history[\"t_id_out_accuracy\"], label = \"Training\")\n",
    "plt.plot(hist.history[\"val_t_id_out_accuracy\"], label = \"Validation\")\n",
    "plt.ylim([-0.1, 1.1])\n",
    "plt.title(\"Accuracy\")\n",
    "plt.legend(fontsize = 30)\n",
    "plt.savefig(\"Images/Histories/NOPREPBiasAcc.png\", format=\"png\", transparent = True,\n",
    "                    dpi = 1000,\n",
    "                    bbox_inches='tight',\n",
    "                    pad_inches=0.5)\n",
    "plt.show()\n",
    "\n",
    "# get performance on each sample\n",
    "sample_dict = {}\n",
    "\n",
    "num_patients = len(np.unique(np.argmax(train_y, axis = 0)))\n",
    "for i in range(num_patients):\n",
    "\n",
    "        # Get data from class i of the training set\n",
    "    d = np.squeeze(train_x[np.argmax(train_y, axis = 1) == i])\n",
    "    y = np.squeeze(train_y[np.argmax(train_y, axis = 1) == i])\n",
    "    lgm = np.squeeze(train_lgm[np.argmax(train_y, axis = 1) == i])\n",
    "    _, train_loss, _, train_acc, train_acc_lgm  = sample_model.evaluate(d, [y, lgm], batch_size = 256)\n",
    "    train_size = int(len(d))\n",
    "\n",
    "        # Get data from class i of the validation set, these spectra have not been seen before\n",
    "    d = np.squeeze(val_x[np.argmax(val_y, axis = 1) == i])\n",
    "    y = np.squeeze(val_y[np.argmax(val_y, axis = 1) == i])\n",
    "    lgm = np.squeeze(val_lgm[np.argmax(val_y, axis = 1) == i])\n",
    "    _, val_loss, _, val_acc, val_acc_lgm  = sample_model.evaluate(d, [y, lgm], batch_size = 256)\n",
    "    val_size = int(len(d))\n",
    "        \n",
    "        # Get data from class i of the test set, these spectra have not been seen before\n",
    "    d = np.squeeze(test_x[np.argmax(test_y, axis = 1) == i])\n",
    "    y = np.squeeze(test_y[np.argmax(test_y, axis = 1) == i])\n",
    "    lgm = np.squeeze(test_lgm[np.argmax(test_y, axis = 1) == i])\n",
    "    _, test_loss, _, test_acc, test_acc_lgm  = sample_model.evaluate(d, [y, lgm], batch_size = 256)\n",
    "    test_size = int(len(d))\n",
    "\n",
    "    sample_dict[i] = np.array([train_acc, train_acc_lgm, train_size, val_acc, val_acc_lgm, val_size, test_acc, test_acc_lgm, test_size])\n",
    "\n",
    "# Save the metrics in a dataframe\n",
    "header = [\"Sample ID\", \"Train Accuracy\", \"Train LGm Accuracy\", \"Train Size\",\n",
    "          \"Validation Accuracy\", \"Validation LGm Accuracy\", \"Validation Size\",\n",
    "          \"Test Accuracy\", \"Test LGm Accuracy\", \"Test Size\"]\n",
    "\n",
    "df = pd.DataFrame.from_dict(sample_dict, columns = header[1:],\n",
    "                                        orient = \"index\")\n",
    "# Round them to two decimals\n",
    "df = df.round(decimals = 2)\n",
    "\n",
    "# Cast the number of spectra to integer, looks nicer than the decimal form with .0 after each number\n",
    "df[\"Train Size\"] = df[\"Train Size\"].apply(lambda x: int(x))\n",
    "df[\"Validation Size\"] = df[\"Validation Size\"].apply(lambda x: int(x))\n",
    "df[\"Test Size\"] = df[\"Test Size\"].apply(lambda x: int(x))\n",
    "\n",
    "# Styling for convertion into latex format\n",
    "df.style.set_table_styles([\n",
    "    {'selector': 'toprule', 'props': ':hline;'},\n",
    "    {'selector': 'midrule', 'props': ':hline;'},\n",
    "    {'selector': 'bottomrule', 'props': ':hline;'},\n",
    "], overwrite=False)\n",
    "\n",
    "latex = df.style.format(decimal=',', thousands='.', precision=2).to_latex(clines=\"all;data\",  column_format=\"|l|l|\")\n",
    "\n",
    "# Print the latex table, can be copied into the editor\n",
    "print(latex)\n",
    "\n",
    "# Save metrics\n",
    "np.save(\"Results/RADAREffectOnBias/NOPREPtestaccuracy.npy\", df[\"Test Accuracy\"].values)\n",
    "np.save(\"Results/RADAREffectOnBias/NOPREPallaccuracies.npy\", df.values)\n",
    "\n",
    "del train_x\n",
    "del train_y\n",
    "del val_x\n",
    "del val_y\n",
    "del test_x\n",
    "del test_y\n",
    "del sample_model\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db9b41af-37da-46f1-87ec-76e446d14ccf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Redo experiment for the manually corrected data\n",
    "\n",
    "gc.collect()\n",
    "p = \"Data/\"\n",
    "train_x = np.load(p + \"train_x_MANUAL.npy\")\n",
    "test_x = np.load(p + \"test_x_MANUAL.npy\")\n",
    "val_x = np.load(p + \"val_x_MANUAL.npy\")\n",
    "\n",
    "train_y = np.load(p + \"train_y_46.npy\")\n",
    "test_y = np.load(p + \"test_y_46.npy\")\n",
    "val_y = np.load(p + \"val_y_46.npy\")\n",
    "\n",
    "train_lgm = np.load(p + \"train_lgm.npy\")\n",
    "test_lgm = np.load(p + \"test_lgm.npy\")\n",
    "val_lgm = np.load(p + \"val_lgm.npy\")\n",
    "\n",
    "\n",
    "# Binary encoding from lgm to mutant vs. wildtype\n",
    "train_lgm = np.argmax(train_lgm, axis = 1)\n",
    "test_lgm = np.argmax(test_lgm, axis = 1)\n",
    "val_lgm = np.argmax(val_lgm, axis = 1)\n",
    "\n",
    "train_lgm = np.where(train_lgm > 2, 0, 1)\n",
    "test_lgm = np.where(test_lgm > 2, 0, 1)\n",
    "val_lgm = np.where(val_lgm > 2, 0, 1)\n",
    "\n",
    "eye = np.eye(2)\n",
    "\n",
    "train_lgm = eye[train_lgm]\n",
    "val_lgm = eye[val_lgm]\n",
    "test_lgm = eye[test_lgm]\n",
    "\n",
    "\n",
    "np.random.seed(0)\n",
    "ix = np.arange(len(train_x))\n",
    "np.random.shuffle(ix)\n",
    "train_x = train_x[ix]\n",
    "train_y = train_y[ix]\n",
    "train_lgm = train_lgm[ix]\n",
    "\n",
    "# Train and save the model\n",
    "reset_seed()\n",
    "sample_model = make_split_model(lr, out_dims = [len(train_y[0]), len(train_lgm[0])])\n",
    "sample_model.summary()\n",
    "hist = sample_model.fit(train_x, [train_y, train_lgm], epochs = epochs,\n",
    "                        batch_size = batch_size, validation_data = (val_x, [val_y, val_lgm]), callbacks = [early_stop])\n",
    "sample_model.save_weights(\"Models\\MANUAL_Bias_quantifier.h5\")\n",
    "\n",
    "\n",
    "# or save to csv: \n",
    "hist_csv_file = 'MANUAL_bias_history.csv'\n",
    "with open(hist_csv_file, mode='w') as f:\n",
    "    hist_df.to_csv(f)\n",
    "\n",
    "# Show metrics on the datasets\n",
    "print(\"ID balanced accuracy\")\n",
    "preds = np.argmax(sample_model.predict(train_x)[0], axis = 1)\n",
    "print(\"Train:\", np.round(balanced_accuracy_score(np.argmax(train_y, axis = 1), preds), 2))\n",
    "\n",
    "preds = np.argmax(sample_model.predict(val_x)[0], axis = 1)\n",
    "print(\"Validation:\", np.round(balanced_accuracy_score(np.argmax(val_y, axis = 1), preds), 2))\n",
    "\n",
    "preds = np.argmax(sample_model.predict(test_x)[0], axis = 1)\n",
    "print(\"Test:\", np.round(balanced_accuracy_score(np.argmax(test_y, axis = 1), preds), 2))\n",
    "\n",
    "print(\"LGM balanced accuracy\")\n",
    "preds = np.argmax(sample_model.predict(train_x)[1], axis = 1)\n",
    "print(\"Train:\", np.round(balanced_accuracy_score(np.argmax(train_lgm, axis = 1), preds), 2))\n",
    "\n",
    "preds = np.argmax(sample_model.predict(val_x)[1], axis = 1)\n",
    "print(\"Validation:\", np.round(balanced_accuracy_score(np.argmax(val_lgm, axis = 1), preds), 2))\n",
    "\n",
    "preds = np.argmax(sample_model.predict(test_x)[1], axis = 1)\n",
    "print(\"Test:\", np.round(balanced_accuracy_score(np.argmax(test_lgm, axis = 1), preds), 2))\n",
    "\n",
    "\n",
    "# Plot training process\n",
    "\n",
    "# Loss\n",
    "plt.rcParams.update({'font.size': 40})\n",
    "plt.rcParams[\"font.family\"] = \"Times New Roman\"\n",
    "plt.plot(hist.history[\"loss\"], label = \"Training\")\n",
    "plt.plot(hist.history[\"val_loss\"], label = \"Validation\")\n",
    "plt.title(\"Loss\")\n",
    "maxim = np.max(hist.history[\"val_loss\"])\n",
    "\n",
    "plt.legend(fontsize = 30)\n",
    "plt.savefig(\"Images/Histories/MANUALBiasLoss.png\", format=\"png\", transparent = True,\n",
    "                    dpi = 1000,\n",
    "                    bbox_inches='tight',\n",
    "                    pad_inches=0.5)\n",
    "plt.show()\n",
    "\n",
    "# Accuracies\n",
    "plt.plot(hist.history[\"t_id_out_accuracy\"], label = \"Training\")\n",
    "plt.plot(hist.history[\"val_t_id_out_accuracy\"], label = \"Validation\")\n",
    "plt.ylim([-0.1, 1.1])\n",
    "plt.title(\"Accuracy\")\n",
    "plt.legend(fontsize = 30)\n",
    "plt.savefig(\"Images/Histories/MANUALBiasAcc.png\", format=\"png\", transparent = True,\n",
    "                    dpi = 1000,\n",
    "                    bbox_inches='tight',\n",
    "                    pad_inches=0.5)\n",
    "plt.show()\n",
    "\n",
    "# get performance on each sample\n",
    "sample_dict = {}\n",
    "\n",
    "num_patients = len(np.unique(np.argmax(train_y, axis = 0)))\n",
    "for i in range(num_patients):\n",
    "\n",
    "        # Get data from class i of the training set\n",
    "    d = np.squeeze(train_x[np.argmax(train_y, axis = 1) == i])\n",
    "    y = np.squeeze(train_y[np.argmax(train_y, axis = 1) == i])\n",
    "    lgm = np.squeeze(train_lgm[np.argmax(train_y, axis = 1) == i])\n",
    "    _, train_loss, _, train_acc, train_acc_lgm  = sample_model.evaluate(d, [y, lgm], batch_size = 256)\n",
    "    train_size = int(len(d))\n",
    "\n",
    "        # Get data from class i of the validation set, these spectra have not been seen before\n",
    "    d = np.squeeze(val_x[np.argmax(val_y, axis = 1) == i])\n",
    "    y = np.squeeze(val_y[np.argmax(val_y, axis = 1) == i])\n",
    "    lgm = np.squeeze(val_lgm[np.argmax(val_y, axis = 1) == i])\n",
    "    _, val_loss, _, val_acc, val_acc_lgm  = sample_model.evaluate(d, [y, lgm], batch_size = 256)\n",
    "    val_size = int(len(d))\n",
    "        \n",
    "        # Get data from class i of the test set, these spectra have not been seen before\n",
    "    d = np.squeeze(test_x[np.argmax(test_y, axis = 1) == i])\n",
    "    y = np.squeeze(test_y[np.argmax(test_y, axis = 1) == i])\n",
    "    lgm = np.squeeze(test_lgm[np.argmax(test_y, axis = 1) == i])\n",
    "    _, test_loss, _, test_acc, test_acc_lgm  = sample_model.evaluate(d, [y, lgm], batch_size = 256)\n",
    "    test_size = int(len(d))\n",
    "\n",
    "    sample_dict[i] = np.array([train_acc, train_acc_lgm, train_size, val_acc, val_acc_lgm, val_size, test_acc, test_acc_lgm, test_size])\n",
    "\n",
    "# Save the metrics in a dataframe\n",
    "header = [\"Sample ID\", \"Train Accuracy\", \"Train LGm Accuracy\", \"Train Size\",\n",
    "          \"Validation Accuracy\", \"Validation LGm Accuracy\", \"Validation Size\",\n",
    "          \"Test Accuracy\", \"Test LGm Accuracy\", \"Test Size\"]\n",
    "\n",
    "df = pd.DataFrame.from_dict(sample_dict, columns = header[1:],\n",
    "                                        orient = \"index\")\n",
    "# Round them to two decimals\n",
    "df = df.round(decimals = 2)\n",
    "\n",
    "# Cast the number of spectra to integer, looks nicer than the decimal form with .0 after each number\n",
    "df[\"Train Size\"] = df[\"Train Size\"].apply(lambda x: int(x))\n",
    "df[\"Validation Size\"] = df[\"Validation Size\"].apply(lambda x: int(x))\n",
    "df[\"Test Size\"] = df[\"Test Size\"].apply(lambda x: int(x))\n",
    "\n",
    "# Styling for convertion into latex format\n",
    "df.style.set_table_styles([\n",
    "    {'selector': 'toprule', 'props': ':hline;'},\n",
    "    {'selector': 'midrule', 'props': ':hline;'},\n",
    "    {'selector': 'bottomrule', 'props': ':hline;'},\n",
    "], overwrite=False)\n",
    "\n",
    "latex = df.style.format(decimal=',', thousands='.', precision=2).to_latex(clines=\"all;data\",  column_format=\"|l|l|\")\n",
    "\n",
    "# Print the latex table, can be copied into the editor\n",
    "print(latex)\n",
    "\n",
    "# Save metrics\n",
    "np.save(\"Results/RADAREffectOnBias/MANUALtestaccuracy.npy\", df[\"Test Accuracy\"].values)\n",
    "np.save(\"Results/RADAREffectOnBias/MANUALallaccuracies.npy\", df.values)\n",
    "\n",
    "del train_x\n",
    "del train_y\n",
    "del val_x\n",
    "del val_y\n",
    "del test_x\n",
    "del test_y\n",
    "del sample_model\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "656a9d4f-c75e-4eeb-bd45-2af3843836b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect()\n",
    "# Redo the experiment using RADAR processed data\n",
    "\n",
    "p = \"Data/\"\n",
    "train_x = np.load(p + \"train_x_RADAR.npy\")\n",
    "test_x = np.load(p + \"test_x_RADAR.npy\")\n",
    "val_x = np.load(p + \"val_x_RADAR.npy\")\n",
    "\n",
    "train_y = np.load(p + \"train_y_46.npy\")\n",
    "test_y = np.load(p + \"test_y_46.npy\")\n",
    "val_y = np.load(p + \"val_y_46.npy\")\n",
    "\n",
    "train_lgm = np.load(p + \"train_lgm.npy\")\n",
    "test_lgm = np.load(p + \"test_lgm.npy\")\n",
    "val_lgm = np.load(p + \"val_lgm.npy\")\n",
    "\n",
    "# Binary encoding from lgm to mutant vs. wildtype\n",
    "train_lgm = np.argmax(train_lgm, axis = 1)\n",
    "test_lgm = np.argmax(test_lgm, axis = 1)\n",
    "val_lgm = np.argmax(val_lgm, axis = 1)\n",
    "\n",
    "train_lgm = np.where(train_lgm > 2, 0, 1)\n",
    "test_lgm = np.where(test_lgm > 2, 0, 1)\n",
    "val_lgm = np.where(val_lgm > 2, 0, 1)\n",
    "\n",
    "eye = np.eye(2)\n",
    "\n",
    "train_lgm = eye[train_lgm]\n",
    "val_lgm = eye[val_lgm]\n",
    "test_lgm = eye[test_lgm]\n",
    "\n",
    "\n",
    "np.random.seed(0)\n",
    "ix = np.arange(len(train_x))\n",
    "np.random.shuffle(ix)\n",
    "train_x = train_x[ix]\n",
    "train_y = train_y[ix]\n",
    "train_lgm = train_lgm[ix]\n",
    "\n",
    "# Train and save the model\n",
    "reset_seed()\n",
    "sample_model = make_split_model(lr, out_dims = [len(train_y[0]), len(train_lgm[0])])\n",
    "sample_model.summary()\n",
    "hist = sample_model.fit(train_x, [train_y, train_lgm], epochs = epochs,\n",
    "                        batch_size = batch_size, validation_data = (val_x, [val_y, val_lgm]), callbacks = [early_stop])\n",
    "sample_model.save_weights(\"Models\\RADAR_Bias_quantifier.h5\")\n",
    "\n",
    "\n",
    "# or save to csv: \n",
    "hist_csv_file = 'RADAR_bias_history.csv'\n",
    "with open(hist_csv_file, mode='w') as f:\n",
    "    hist_df.to_csv(f)\n",
    "    \n",
    "# Show metrics on the datasets\n",
    "print(\"ID balanced accuracy\")\n",
    "preds = np.argmax(sample_model.predict(train_x)[0], axis = 1)\n",
    "print(\"Train:\", np.round(balanced_accuracy_score(np.argmax(train_y, axis = 1), preds), 2))\n",
    "\n",
    "preds = np.argmax(sample_model.predict(val_x)[0], axis = 1)\n",
    "print(\"Validation:\", np.round(balanced_accuracy_score(np.argmax(val_y, axis = 1), preds), 2))\n",
    "\n",
    "preds = np.argmax(sample_model.predict(test_x)[0], axis = 1)\n",
    "print(\"Test:\", np.round(balanced_accuracy_score(np.argmax(test_y, axis = 1), preds), 2))\n",
    "\n",
    "print(\"LGM balanced accuracy\")\n",
    "preds = np.argmax(sample_model.predict(train_x)[1], axis = 1)\n",
    "print(\"Train:\", np.round(balanced_accuracy_score(np.argmax(train_lgm, axis = 1), preds), 2))\n",
    "\n",
    "preds = np.argmax(sample_model.predict(val_x)[1], axis = 1)\n",
    "print(\"Validation:\", np.round(balanced_accuracy_score(np.argmax(val_lgm, axis = 1), preds), 2))\n",
    "\n",
    "preds = np.argmax(sample_model.predict(test_x)[1], axis = 1)\n",
    "print(\"Test:\", np.round(balanced_accuracy_score(np.argmax(test_lgm, axis = 1), preds), 2))\n",
    "\n",
    "\n",
    "# Plot training process\n",
    "plt.rcParams.update({'font.size': 40})\n",
    "plt.rcParams[\"font.family\"] = \"Times New Roman\"\n",
    "plt.plot(hist.history[\"loss\"], label = \"Training\")\n",
    "plt.plot(hist.history[\"val_loss\"], label = \"Validation\")\n",
    "plt.title(\"Loss\")\n",
    "maxim = np.max(hist.history[\"val_loss\"])\n",
    "\n",
    "plt.legend(fontsize = 30)\n",
    "plt.savefig(\"Images/Histories/RADARBiasLoss.png\", format=\"png\", transparent = True,\n",
    "                    dpi = 1000,\n",
    "                    bbox_inches='tight',\n",
    "                    pad_inches=0.5)\n",
    "plt.show()\n",
    "\n",
    "plt.plot(hist.history[\"t_id_out_accuracy\"], label = \"Training\")\n",
    "plt.plot(hist.history[\"val_t_id_out_accuracy\"], label = \"Validation\")\n",
    "plt.ylim([-0.1, 1.1])\n",
    "plt.title(\"Accuracy\")\n",
    "plt.legend(fontsize = 30)\n",
    "plt.savefig(\"Images/Histories/RADARBiasAcc.png\", format=\"png\", transparent = True,\n",
    "                    dpi = 1000,\n",
    "                    bbox_inches='tight',\n",
    "                    pad_inches=0.5)\n",
    "plt.show()\n",
    "\n",
    "# get performance on each sample\n",
    "sample_dict = {}\n",
    "\n",
    "num_patients = len(np.unique(np.argmax(train_y, axis = 0)))\n",
    "for i in range(num_patients):\n",
    "\n",
    "        # Get data from class i of the training set\n",
    "    d = np.squeeze(train_x[np.argmax(train_y, axis = 1) == i])\n",
    "    y = np.squeeze(train_y[np.argmax(train_y, axis = 1) == i])\n",
    "    lgm = np.squeeze(train_lgm[np.argmax(train_y, axis = 1) == i])\n",
    "    _, train_loss, _, train_acc, train_acc_lgm  = sample_model.evaluate(d, [y, lgm], batch_size = 256)\n",
    "    train_size = int(len(d))\n",
    "\n",
    "        # Get data from class i of the validation set, these spectra have not been seen before\n",
    "    d = np.squeeze(val_x[np.argmax(val_y, axis = 1) == i])\n",
    "    y = np.squeeze(val_y[np.argmax(val_y, axis = 1) == i])\n",
    "    lgm = np.squeeze(val_lgm[np.argmax(val_y, axis = 1) == i])\n",
    "    _, val_loss, _, val_acc, val_acc_lgm  = sample_model.evaluate(d, [y, lgm], batch_size = 256)\n",
    "    val_size = int(len(d))\n",
    "        \n",
    "        # Get data from class i of the test set, these spectra have not been seen before\n",
    "    d = np.squeeze(test_x[np.argmax(test_y, axis = 1) == i])\n",
    "    y = np.squeeze(test_y[np.argmax(test_y, axis = 1) == i])\n",
    "    lgm = np.squeeze(test_lgm[np.argmax(test_y, axis = 1) == i])\n",
    "    _, test_loss, _, test_acc, test_acc_lgm  = sample_model.evaluate(d, [y, lgm], batch_size = 256)\n",
    "    test_size = int(len(d))\n",
    "\n",
    "    sample_dict[i] = np.array([train_acc, train_acc_lgm, train_size, val_acc, val_acc_lgm, val_size, test_acc, test_acc_lgm, test_size])\n",
    "\n",
    "# Save the metrics in a dataframe\n",
    "header = [\"Sample ID\", \"Train Accuracy\", \"Train LGm Accuracy\", \"Train Size\",\n",
    "          \"Validation Accuracy\", \"Validation LGm Accuracy\", \"Validation Size\",\n",
    "          \"Test Accuracy\", \"Test LGm Accuracy\", \"Test Size\"]\n",
    "\n",
    "df = pd.DataFrame.from_dict(sample_dict, columns = header[1:],\n",
    "                                        orient = \"index\")\n",
    "\n",
    "df = df.round(decimals = 2)\n",
    "\n",
    "df[\"Train Size\"] = df[\"Train Size\"].apply(lambda x: int(x))\n",
    "df[\"Validation Size\"] = df[\"Validation Size\"].apply(lambda x: int(x))\n",
    "df[\"Test Size\"] = df[\"Test Size\"].apply(lambda x: int(x))\n",
    "\n",
    "\n",
    "df.style.set_table_styles([\n",
    "    {'selector': 'toprule', 'props': ':hline;'},\n",
    "    {'selector': 'midrule', 'props': ':hline;'},\n",
    "    {'selector': 'bottomrule', 'props': ':hline;'},\n",
    "], overwrite=False)\n",
    "latex = df.style.format(decimal=',', thousands='.', precision=2).to_latex(clines=\"all;data\",  column_format=\"|l|l|\")\n",
    "print(latex)\n",
    "\n",
    "np.save(\"Results/RADAREffectOnBias/RADARtestaccuracy.npy\", df[\"Test Accuracy\"].values)\n",
    "np.save(\"Results/RADAREffectOnBias/RADARallaccuracies.npy\", df.values)\n",
    "\n",
    "del train_x\n",
    "del train_y\n",
    "del val_x\n",
    "del val_y\n",
    "del test_x\n",
    "del test_y\n",
    "del sample_model\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "438a5796-0431-486a-ab2d-ba5d7a107e76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the saved metrics\n",
    "raw_test_acc = np.load(\"Results/RADAREffectOnBias/NOPREPtestaccuracy.npy\")\n",
    "prep_test_acc = np.load(\"Results/RADAREffectOnBias/RADARtestaccuracy.npy\")\n",
    "manual_test_acc = np.load(\"Results/RADAREffectOnBias/MANUALtestaccuracy.npy\")\n",
    "\n",
    "# Set image parameters\n",
    "plt.rcParams.update({'font.size': 35})\n",
    "plt.rcParams[\"font.family\"] = \"Times New Roman\"\n",
    "\n",
    "# Sort the samples according to RADAR performance\n",
    "sorting = np.argsort(manual_test_acc)\n",
    "\n",
    "plt.figure(figsize = (10, 5))\n",
    "\n",
    "mean_diff = np.mean(np.array(raw_test_acc) - np.array(manual_test_acc))\n",
    "print(\"Mean improvement between the raw data and manually processed data:\", np.round(mean_diff, 2))\n",
    "\n",
    "mean_diff = np.mean(np.array(raw_test_acc) - np.array(prep_test_acc))\n",
    "print(\"Mean improvement between the raw data and RADAR processed data:\", np.round(mean_diff, 2))\n",
    "\n",
    "num_unique_labels = 46\n",
    "plt.scatter(np.arange(num_unique_labels), np.array(raw_test_acc)[sorting], label = \"Raw data:\" + str(np.round(np.mean(raw_test_acc), 2)))\n",
    "plt.scatter(np.arange(num_unique_labels), np.array(manual_test_acc)[sorting], label = \"Manually processed:\" + str(np.round(np.mean(manual_test_acc), 2)))\n",
    "plt.scatter(np.arange(num_unique_labels), np.array(prep_test_acc)[sorting], label = \"RADAR:\" + str(np.round(np.mean(prep_test_acc), 2)))\n",
    "plt.ylim([-0.1, 1.1])\n",
    "plt.legend(fontsize = 15)\n",
    "\n",
    "plt.savefig(\"Images/Histories/TestACCComparison_rawVprep.png\", format=\"png\", transparent = True,\n",
    "                    dpi = 1000,\n",
    "                    bbox_inches='tight',\n",
    "                    pad_inches=0.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1f83fa6-93e3-48e9-b6bc-a35d5b624e68",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import PrecisionRecallDisplay, auc\n",
    "# load the models and get their per-spectrum predictions\n",
    "p = \"Data/\"\n",
    "# Raw\n",
    "sample_model = make_split_model(lr, out_dims = [46, 2])\n",
    "sample_model.load_weights(\"Models\\RAW_Bias_quantifier.h5\")\n",
    "\n",
    "test_x = np.load(p + \"test_x.npy\")\n",
    "test_y = np.load(p + \"test_y_46.npy\")\n",
    "y_t = np.argmax(test_y, axis = 1)\n",
    "\n",
    "y_p = sample_model.predict(test_x, batch_size = 128)\n",
    "y_p_id = np.argmax(y_p[0], axis = 1)\n",
    "\n",
    "RAW_auc= []\n",
    "for n in np.unique(y_t):\n",
    "    temp_yt = np.where(y_t == n, 1, 0)\n",
    "    temp_yp = np.where(y_p_id == n, 1, 0)\n",
    "    disp = PrecisionRecallDisplay.from_predictions(temp_yt, temp_yp)\n",
    "\n",
    "    RAW_auc.append(auc(disp.recall, disp.precision))\n",
    "\n",
    "np.save(\"Results/RADAREffectOnBias/NOPREPtestauc.npy\", RAW_auc)\n",
    "\n",
    "del test_x, test_y, y_p, y_p_id, y_t\n",
    "gc.collect()\n",
    "\n",
    "# Manual\n",
    "sample_model = make_split_model(lr, out_dims = [46, 2])\n",
    "sample_model.load_weights(\"Models\\MANUAL_Bias_quantifier.h5\")\n",
    "\n",
    "test_x = np.load(p + \"test_x_MANUAL.npy\")\n",
    "test_y = np.load(p + \"test_y_46.npy\")\n",
    "y_t = np.argmax(test_y, axis = 1)\n",
    "\n",
    "y_p = sample_model.predict(test_x, batch_size = 128)\n",
    "y_p_id = np.argmax(y_p[0], axis = 1)\n",
    "\n",
    "\n",
    "MANUAL_auc = []\n",
    "for n in np.unique(y_t):\n",
    "    temp_yt = np.where(y_t == n, 1, 0)\n",
    "    temp_yp = np.where(y_p_id == n, 1, 0)\n",
    "    disp = PrecisionRecallDisplay.from_predictions(temp_yt, temp_yp)\n",
    "\n",
    "    MANUAL_auc.append(auc(disp.recall, disp.precision))\n",
    "\n",
    "np.save(\"Results/RADAREffectOnBias/MANUALtestauc.npy\", MANUAL_auc)\n",
    "\n",
    "del test_x, test_y, y_p, y_p_id, y_t\n",
    "gc.collect()\n",
    "\n",
    "# RADAR\n",
    "sample_model = make_split_model(lr, out_dims = [46, 2])\n",
    "sample_model.load_weights(\"Models\\RADAR_Bias_quantifier.h5\")\n",
    "\n",
    "test_x = np.load(p + \"test_x_RADAR.npy\")\n",
    "test_y = np.load(p + \"test_y_46.npy\")\n",
    "y_t = np.argmax(test_y, axis = 1)\n",
    "\n",
    "y_p = sample_model.predict(test_x, batch_size = 128)\n",
    "y_p_id = np.argmax(y_p[0], axis = 1)\n",
    "\n",
    "RADAR_auc= []\n",
    "for n in np.unique(y_t):\n",
    "    temp_yt = np.where(y_t == n, 1, 0)\n",
    "    temp_yp = np.where(y_p_id == n, 1, 0)\n",
    "    disp = PrecisionRecallDisplay.from_predictions(temp_yt, temp_yp)\n",
    "\n",
    "    RADAR_auc.append(auc(disp.recall, disp.precision))\n",
    "\n",
    "np.save(\"Results/RADAREffectOnBias/RADARtestauc.npy\", RADAR_auc)\n",
    "\n",
    "del test_x, test_y, y_p, y_p_id, y_t\n",
    "gc.collect()\n",
    "\n",
    "plt.show()\n",
    "# Set image parameters\n",
    "plt.rcParams.update({'font.size': 35})\n",
    "plt.rcParams[\"font.family\"] = \"Times New Roman\"\n",
    "\n",
    "# Sort the samples according to MANUAL performance\n",
    "sorting = np.argsort(MANUAL_auc)\n",
    "\n",
    "plt.figure(figsize = (10, 5))\n",
    "\n",
    "\n",
    "\n",
    "mean_diff = np.mean(np.array(RAW_auc) - np.array(MANUAL_auc))\n",
    "print(\"Mean improvement between the raw data and manually processed data:\", np.round(mean_diff, 2))\n",
    "\n",
    "mean_diff = np.mean(np.array(RAW_auc) - np.array(RADAR_auc))\n",
    "print(\"Mean improvement between the raw data and RADAR processed data:\", np.round(mean_diff, 2))\n",
    "num_unique_labels = 46\n",
    "\n",
    "for i in range(len(RAW_auc)):\n",
    "    if np.array(RAW_auc)[sorting][i] >= np.array(MANUAL_auc)[sorting][i]:\n",
    "        plt.plot(i, np.array(RAW_auc)[sorting][i], i, np.array(MANUAL_auc)[sorting][i], color = \"red\")\n",
    "    else:\n",
    "        plt.plot(i, np.array(RAW_auc)[sorting][i], i, np.array(MANUAL_auc)[sorting][i], color = \"green\")\n",
    "\n",
    "plt.scatter(np.arange(num_unique_labels), np.array(RAW_auc)[sorting], label = \"Raw data:\" + str(np.round(np.mean(RAW_auc), 2)))\n",
    "plt.scatter(np.arange(num_unique_labels), np.array(MANUAL_auc)[sorting], label = \"Manually processed:\" + str(np.round(np.mean(MANUAL_auc), 2)))\n",
    "plt.scatter(np.arange(num_unique_labels), np.array(RADAR_auc)[sorting], label = \"RADAR:\" + str(np.round(np.mean(RADAR_auc), 2)))\n",
    "plt.ylim([-0.1, 1.1])\n",
    "plt.legend(fontsize = 15)\n",
    "\n",
    "plt.savefig(\"Images/Histories/TestAUCComparison_rawVprep.png\", format=\"png\", transparent = True,\n",
    "                    dpi = 300,\n",
    "                    bbox_inches='tight',\n",
    "                    pad_inches=0.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c50eafc4-d314-4f10-8eef-e4e273a6433d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2eb0e5e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
